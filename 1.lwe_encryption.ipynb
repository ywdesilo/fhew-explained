{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWE and RLWE encryption\n",
    "## LWE encryption\n",
    "\n",
    "In FHEW-like HE, we use both LWE and RLWE encryption.\n",
    "LWE is for a ciphertext and RLWE is for the core part of bootstrapping so called *blind rotation*.\n",
    "\n",
    "We define LWE encryption as follows\n",
    "$$\\texttt{LWE}_{\\vec{s}, n, q}(m) = (\\beta, \\vec{\\alpha}) \\in \\mathbb{Z}_q^{n+1},$$\n",
    "where $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right> \\in \\mathbb{Z}_q$, and $\\vec{s} \\leftarrow \\chi_{key}$ is a secret key and $\\vec{e} \\leftarrow \\chi_{err}$ is a added noise for security.\n",
    "$\\vec{\\alpha}$ is unifromly sampled in $\\mathbb{Z}_q^n$.\n",
    "\n",
    "Here, we choose $\\chi_{key}$ as binary distribution and $\\chi_{err}$ as a Gaussian distribution with standard deviation $3.2$.\n",
    "\n",
    "Let's make the encryption method. We use pytorch for easy and fast implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use parameter sets $(n, q, \\sigma) = (512, 2048, 3.2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = 3.2\n",
    "n = 512\n",
    "q = 2048"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are generator of key and error.\n",
    "\n",
    "NOTE: Those generators are not secure. You should **NOT** use them in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keygen(dim):\n",
    "    return torch.randint(2, size = (dim,))\n",
    "def errgen(stddev):\n",
    "    e = torch.round(stddev*torch.randn(1))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "def uniform(dim, modulus):\n",
    "    return torch.randint(modulus, size = (dim,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate the secret key $\\vec{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = keygen(n)\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encrypt, we need random part $\\vec{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1273, 1743, 1642, 1249, 1307,  252,   52, 1288, 1809, 1828, 1185, 1945,\n",
       "        1402, 1011,  925, 1596, 1372, 1363,  939,  900,  713,  240, 2023, 1145,\n",
       "         823, 1947, 1770, 1123,  753,  147,  302, 1480, 1391, 1522,  549,  833,\n",
       "        1066,  106,  603,  351,  805, 1325,  842, 1883, 1857, 1029, 1684, 1257,\n",
       "        1772, 1578, 1345,  839,    9,  862, 1650, 1682,  301, 1450, 1203, 1791,\n",
       "         284, 1263, 1711,  923,  534,   99, 1009, 1755, 2002, 1203,  854, 1365,\n",
       "        2042, 1528, 1518, 2028, 1828,  102, 1920, 1607,  412, 1250,  739, 1608,\n",
       "        1775,   13,  463,  448, 1517, 1708, 1892, 1859, 1216, 1912, 1876,  442,\n",
       "         787, 1680, 1395, 1011,  901,  559, 1008,  152, 2005, 1458, 1173, 1883,\n",
       "        1534,  613, 1623, 1209,  172,  443, 1421,   97,  527,  457,  748,   99,\n",
       "          41,  145, 1653, 1315, 1423, 1054, 1767,  885,  948, 1786,  388, 1310,\n",
       "         607,  960, 1606,  713,  956,  468,  628, 1778, 1475, 1359,  341, 1624,\n",
       "        2037, 1221, 1203,  139,  769, 1660,  741,  300,  988,  459,  593,  717,\n",
       "        1366,  702,  822, 1741, 1736,  196, 1212, 1595,  821, 1511,  735, 1831,\n",
       "         643,  746,  654,  835, 1581, 1826,  422,  853,  180,  862, 1135,   21,\n",
       "        1171, 1076,    1, 1877,  918, 1170, 1521, 1500,  309, 1664, 1977, 1262,\n",
       "         484, 1816, 1617,  114, 1305, 1291,   23, 1382,  248, 1198,  437,   12,\n",
       "         789, 2018,  835,  689, 1316, 1834,  900,  661,  562, 1747,  105,  339,\n",
       "        1558, 1662, 1384, 1154,  972,  223,  714,  995, 1173,  861,  759,  328,\n",
       "        1943, 1529,  881, 1659,  617,  356,  336, 2021,    4,  909,  311, 1574,\n",
       "        1304,  360,   79,  272, 1973, 1186,  799, 1654,  225,  599,  146,   41,\n",
       "        1952, 2040,  564,  346,  818, 1675,  941, 1060, 2045,  871, 1021, 1704,\n",
       "         212, 1120, 1528, 1141, 1017,  436, 2033, 1635, 1945,  724,  857,  168,\n",
       "        1379,  863,  482, 1252,  960,  235, 2020,  184,  458, 1335, 1128,  266,\n",
       "        1358,  262,  655,  841,  865, 1739,  396,   64, 1302,  916,   85, 1193,\n",
       "        1997,  524,  106,  512, 1183, 2009, 1961, 1732, 1724,  985, 1021, 1474,\n",
       "         148,  489,  662, 1180, 2008,  884,  670,  409, 1632, 2039, 1417,  703,\n",
       "         562, 1618, 1085,   28,  842,   19,  746, 1008,  678, 1999,  728,  667,\n",
       "        2001,  124, 1076,  914, 1701,   44,  263, 2035,  734,  390,  110,  925,\n",
       "         472,   50, 1106, 1131,  304, 1523, 1832, 1343,  456,  680, 1310, 1296,\n",
       "        1481, 1522, 1144, 1169, 1065,  246,  729, 1634,  747, 1425, 1984,   16,\n",
       "        1661,  713, 1998,  752, 1971,  395,  952, 1310, 1260,  846, 1738, 1923,\n",
       "         796,  815, 1051,  226,  431, 1745, 1496,  709,  828,  419,  701,  645,\n",
       "         192,  395, 1040,  318,  487, 1483, 1760, 1576, 1997,  833, 1011, 1604,\n",
       "          67,  486, 1185, 1903,  288,   30, 1123,  836, 1024, 1556, 1870, 1210,\n",
       "        1248,  100, 1675,  487,  101,  809,  636,  486, 1286,  870,  362,  306,\n",
       "         879,  476,  324, 1494,  668, 1972, 1697, 1351, 1288, 1105, 2021, 1043,\n",
       "         717,  161, 1546, 1371,  470, 1928,   28,  790, 1714, 1691, 1381,  717,\n",
       "        1804, 1929, 1003, 2012,  827, 1531,  928, 1560,  166, 1371, 1270, 1467,\n",
       "        1322, 1749, 1142, 1709, 1674, 1402, 1981, 1009,  302,  206, 1012,   30,\n",
       "        2047,  900, 1213,  669,  951,   62, 1766, 1481, 1832,  937, 1581,  336,\n",
       "        1328,  772, 2023,  215, 1192,  549,  670,  451,  681,  979, 1961, 1638,\n",
       "         666, 1797,  880, 1214,  619,  869,  136, 1389])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = uniform(n, q)\n",
    "alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calaulate $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$ for encryption.\n",
    "\n",
    "Let the message we are encrypting is a binary value e,g, $m = 1$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "\n",
    "beta %= q\n",
    "\n",
    "beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By *LWE assumption* $\\beta$ should look like a random value.\n",
    "Now the pair $(\\beta, \\vec{\\alpha})$ is our ciphertext.\n",
    "\n",
    "Let's decrypt the ciphertext above.\n",
    "\n",
    "As $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$, we can find $m + e = \\beta + \\left< \\vec{\\alpha}, \\vec{s} \\right>$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are very lucky, you might get the decrypted value.\n",
    "```\n",
    ">>> m_decrypted\n",
    "tensor(1)\n",
    "```\n",
    "But, if you run the code once, you will get other value.\n",
    "Note here that we get $m+e$ by decryption, *not the exact value* $m$.\n",
    "\n",
    "To make our message safe from the error, we can multiply certain *scaling factor* to our message.\n",
    "\n",
    "Here, let's multiply $q/4$, and encrypt/decrypt again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "# multiply scaling factor q/4 \n",
    "m *= q//4\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "beta %= q\n",
    "\n",
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a value near $m \\cdot q/4 = 512$.\n",
    "Division by $q/4$ and rounding will give us original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescale the message\n",
    "m_decrypted = m_decrypted.to(torch.float)\n",
    "m_decrypted /= q/4.\n",
    "m_decrypted = torch.round(m_decrypted)\n",
    "m_decrypted.to(torch.int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decryption is successful!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LWE encryption function\n",
    "\n",
    "The LWE ciphertext is a pair $(\\beta, \\vec{\\alpha})$.\n",
    "\n",
    "We define the encryptor as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1090),\n",
       " tensor([ 813,  427, 1442, 1080,  773,  305,  513, 1108,  860, 1593,  765,  967,\n",
       "          453, 1686, 1271, 1699, 1192, 1567, 1339, 1099, 1867,  638, 2038, 1655,\n",
       "         1752,   22, 1630,  517,  453, 1724,   27,  733,  681, 1184, 1270, 1813,\n",
       "          561,  788, 1469, 1096, 1737, 1501,  104, 1299, 1763, 1847, 1767, 1450,\n",
       "          694, 1200,  884, 1696, 2018, 1707,  429, 1499,  454, 1903, 1842, 1956,\n",
       "         1335, 1261,  141, 1770, 1662,  734,  836, 1805,    8,  209, 1884,  505,\n",
       "         1593, 1508, 1635, 1095, 1331, 1004, 1392,  350, 1072,  125, 1651,  638,\n",
       "         2036, 1821, 2030,  943, 1309, 1453, 1887,  229, 1817,  833, 1164,  433,\n",
       "         1632, 1459, 1786,  135,  922, 1519,   86,  646, 1487, 1698,  461, 1527,\n",
       "         1354, 1421, 1902, 1626,  885, 1996,  733, 1744,    6, 1569, 1753,  485,\n",
       "         1820,  658, 1129,  659, 1139, 1058, 1320, 1703, 1949,  345,  783,  406,\n",
       "          415, 1356,  554, 1834, 1002, 1366, 1917, 1997, 1462,  713,  778, 2001,\n",
       "         1418,  564,  332,  140, 2002,  135,  244,  662, 1354,  441, 2019, 1265,\n",
       "           30, 1020,  994, 1003, 1830, 1352,  326, 1545,  965,  947,  944, 1314,\n",
       "          439,  155, 1292, 1996, 1699,  417,  372,  428,  906,  540,  748,  988,\n",
       "          540, 1940, 1477, 1397, 1638, 1063,  773,  939,  899,  735, 1144,  122,\n",
       "          468, 1824, 1141, 1355,  724, 1452, 1558, 1631,  341,  273, 1194, 1439,\n",
       "         1495, 1306, 1315, 1666, 1347, 1319,  105, 1286, 1734,  184, 1922, 1798,\n",
       "         1506, 1352, 1483,  978,  695, 1841, 1207,  928, 1439, 1528,  298,  691,\n",
       "          126, 1717,  941, 1448,  574,  640,  184, 1150,  451, 1798, 1094, 1636,\n",
       "          947,  794,  388,  849, 1253, 1918, 1130, 1431,  323, 1930,   91, 1759,\n",
       "          173,  824,  392,  868, 1223,  612,  497, 1252, 1443,  728,  783, 1390,\n",
       "          179, 1344, 1814, 1076, 1633, 1825, 1772,  168, 1486,  483, 1553,  936,\n",
       "         2016, 1220, 1711, 1874,  325, 1576,  514, 1376, 1488, 1371, 1348,  476,\n",
       "         1829,  793, 1144, 1535, 1752, 1951, 1474, 1860, 1207,  373, 1784,   84,\n",
       "           94,  841, 1581, 1731, 1417,  152,  264,  577,  766,  996, 1286, 1727,\n",
       "           93, 1142, 1907,   82, 1513,  711,  525,   89,  637, 1307,  259,   50,\n",
       "         1713, 1632,  425,  307, 1555,  102, 1075, 1809,  276,  141, 1046,  614,\n",
       "         2013,  917,  763, 1189,  470, 1942,  626,  371,   47, 1631, 1509,  401,\n",
       "         1953,   25,  971, 1024,  766, 1827,  188,  879, 1556,  387,  251, 1680,\n",
       "          371,  706,  699,  172,  763, 2009,  452, 1078, 1082, 1886, 1145, 1363,\n",
       "         1481, 1000,  178, 1692, 1348,  117,  298, 1050,   16,  136, 1490,  191,\n",
       "         1441,  424, 1964, 1063,  670, 1393, 1835,  427,  456, 1638, 1391,  682,\n",
       "         1975, 1383,  297, 1575,    9,  205,  612,  897, 1475, 1377,  894, 1695,\n",
       "          165, 1256,  691, 1929, 1700, 1327,  792, 1491, 1472, 1072,  307, 1014,\n",
       "          981, 1017, 1994,  735, 1066,   74,  163,  346, 1104,  276,  708,  790,\n",
       "          175,   11,   24,  336, 1802, 1921, 1143, 2016,  161, 1874, 1418,  286,\n",
       "         1543,    7, 1940,  829, 1390, 1257,  461, 1959,  952, 1775, 1056,  772,\n",
       "         1466,  183, 1792,  210, 1494, 1641,  715, 1057, 1051, 1593, 1028,  228,\n",
       "         1975,  610,  339, 1319, 1393,  233, 1162, 1511, 1869,  598,  593,  655,\n",
       "         1602,  617,  491, 1899, 1175, 1068,  268,  812,  358, 1884, 1140, 1275,\n",
       "          644, 1700, 1350, 1294,  923, 1498, 1141,  499, 1518,  184,  797,  928,\n",
       "         1983,  881,   23,  511,  349, 1150, 1289, 1009]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encryptLWE(message, dim, modulus, key):\n",
    "    alpha = uniform(dim, modulus)\n",
    "\n",
    "    beta = message * modulus//4 - torch.dot(alpha, key)\n",
    "    e = errgen(stddev)\n",
    "    beta += e\n",
    "    beta %= modulus\n",
    "\n",
    "    return (beta, alpha)\n",
    "\n",
    "\n",
    "ct = encryptLWE(1, n, q, s)\n",
    "ct    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decryptLWE(ct, key, modulus):\n",
    "    beta, alpha = ct\n",
    "    m_dec = beta + torch.dot(alpha, key)\n",
    "    m_dec %= modulus\n",
    "\n",
    "    m_dec = m_dec.to(torch.float)\n",
    "    m_dec /= modulus/4.\n",
    "    m_dec = torch.round(m_dec)\n",
    "    return m_dec.to(torch.int)\n",
    "\n",
    "decryptLWE(ct, s, q)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
