{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LWE and RLWE encryption\n",
    "## 1-1. LWE encryption\n",
    "\n",
    "In FHEW-like HE, we use both LWE and RLWE encryption.\n",
    "LWE is for a ciphertext and RLWE is for the core part of bootstrapping so called *blind rotation*.\n",
    "\n",
    "We define LWE encryption as follows\n",
    "$$\\textsf{LWE}_{\\vec{s}, n, q}(m) = (\\beta, \\vec{\\alpha}) \\in \\mathbb{Z}_q^{n+1},$$\n",
    "where $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right> \\in \\mathbb{Z}_q$, and $\\vec{s} \\leftarrow \\chi_{key}$ is a secret key and $\\vec{e} \\leftarrow \\chi_{err}$ is a added noise for security.\n",
    "$\\vec{\\alpha}$ is unifromly sampled in $\\mathbb{Z}_q^n$.\n",
    "\n",
    "Here, we choose $\\chi_{key}$ as binary distribution and $\\chi_{err}$ as a Gaussian distribution with standard deviation $3.2$.\n",
    "\n",
    "Let's make the encryption method. We use [pytorch](https://pytorch.org/) for easy and fast implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use parameter sets $(n, q, \\sigma) = (512, 2048, 3.2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = 3.2\n",
    "n = 512\n",
    "q = 2048"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are generator of key and error.\n",
    "\n",
    "NOTE: Those generators are not secure. You should **NOT** use them in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keygen(dim):\n",
    "    return torch.randint(2, size = (dim,))\n",
    "\n",
    "def errgen(stddev):\n",
    "    e = torch.round(stddev*torch.randn(1))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "def uniform(dim, modulus):\n",
    "    return torch.randint(modulus, size = (dim,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate the secret key $\\vec{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = keygen(n)\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encrypt, we need random part $\\vec{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2046,  458, 1484,  884,  417, 1187,   70, 1243,  598,  476, 1630, 1536,\n",
       "        1796,  411,  747, 1870,  718,  946, 1535,   75,  790, 1915, 1783,  700,\n",
       "        1553,  191,  839, 1342,  786, 1596, 1884,  440,    3, 1184,  971,  215,\n",
       "        1890,  677, 1550, 1968,   43, 1937, 1647,  588, 1385,   86,    8,   76,\n",
       "        1430, 1221, 1397, 1248,  179, 1750,  310,  729, 1449,  800, 1889,  629,\n",
       "         810,  440,  739,  369, 1795, 1997,  629, 1528, 1365,  866,   81,  702,\n",
       "        1889, 1200,   79, 1225, 1920,  211, 1905, 1014, 1072,  797,  620,  971,\n",
       "        1594,  258, 1829, 1268, 1701,  530, 1123,  279, 1697,  660, 1366, 1414,\n",
       "         311,   86, 2018, 1795, 1592, 1574, 1429, 1968, 1750,  198,  985,  785,\n",
       "        1254, 1870, 1643, 1626, 1037, 1772,  173,  633, 1184, 1819, 1206,  982,\n",
       "        1100,   17, 1778,  879, 1158,  150,  181, 1755, 1787, 1473, 1613, 1949,\n",
       "         447, 1257,  475,  459, 1710,  376,   18,  487,   71, 1459,  394, 1573,\n",
       "        1795,  290,  454, 1606, 1472, 1038,  404, 1367,  282, 1410,  643, 1575,\n",
       "        1989,   54, 1464, 1372,  797, 1508, 1811,  423, 1861,  314,  785, 1867,\n",
       "         877,  418,  166,  232,  511,  282, 1708,  858,  970, 1095,  261,  311,\n",
       "         450,  564, 1676,  732, 1161, 1008, 1988,  756, 1460,  473, 1952,  608,\n",
       "         633, 1483, 1152, 1210,  178,  550,  341,  808,  784, 1742,  493,  401,\n",
       "        1260, 1025, 1607, 1199, 1907,  651, 1841, 1386,  992,  787, 1552, 1942,\n",
       "         625, 1004, 1648,  603,  155,  902, 1651,  424, 1362, 1461,  668, 1957,\n",
       "        1085,  412, 1995, 1243, 1463,  392, 1596,  454, 1312, 1301,  236, 1152,\n",
       "        1553, 2025,  870, 1347, 1728,  592,  143, 1664,  505, 1922, 1406, 1924,\n",
       "        1122,  321,  908, 1819, 1371, 1953, 1228,  853, 1125, 1421,  692, 1541,\n",
       "          25,  834,  291,   45,  116, 1252,  911, 1912, 1515, 1405,  933, 1335,\n",
       "        1080,  193,  727, 1137, 1194, 1977, 1615, 1511,  666, 1408, 1835,   99,\n",
       "         431, 1869,  452,  209,  908,  718, 1113,  786, 1322,  349, 1609,   79,\n",
       "        1852,  970, 1698, 1911,   24, 1944,  730,  690,  915,  698, 1756, 1870,\n",
       "         788, 1945,   73, 1696,   43, 1751,   20,  834, 1652, 2028,  451,  525,\n",
       "        1256, 1517,  229, 1403,  152,  972, 1354,  109,  355,  891, 1819, 1362,\n",
       "         314, 1132, 1748, 1021,  154,  793,  454, 1270, 1595, 1186,  454,  778,\n",
       "        1209,  796,  501, 1004,  807,    9,   45,  122,  968,  905,  882,  481,\n",
       "         987, 1943,  397,  568, 1400, 1313,  627,  117, 1019,  470,   64,  644,\n",
       "         579,  664, 1165,  338, 2019, 1000, 1732, 1699,  320,  722,  252, 1507,\n",
       "        1640, 1363, 1378,  195, 1078,  125,  984,  610, 1019,  385, 1155, 1726,\n",
       "         572,   69,  767, 1062, 1409, 1429,  944, 2045, 1709,  911, 1420, 1892,\n",
       "        1420, 1483, 1026, 1290, 1394, 1574, 1338,  113,  519,  260,  364,  773,\n",
       "         628, 1602, 1332,  132, 1954,  802, 1672,  480,   94, 1991,  293,  663,\n",
       "        1035, 1569, 1707,   11, 1710,  806, 1029, 1530, 1671, 1749, 1179, 1574,\n",
       "         502, 1841,  830,  581, 1808, 1529, 1900, 1740, 2014,  890,  289,   19,\n",
       "         806, 2021, 1505, 1757,  499,  612,   57, 1649, 1756, 1697,  311,  105,\n",
       "         240,  545,  867, 1788,  725,  158, 1326, 1409, 1247,  616,  452,   67,\n",
       "         142, 1709, 1552,  354,  463,  211, 1339,   71,  993, 1622, 1024,   88,\n",
       "        1116, 1901,  548,  952, 1497,  914, 1717, 1257, 1748, 1043, 2014,  119,\n",
       "        1709,    1,  401, 1249, 1829,  393,  248, 1683])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = uniform(n, q)\n",
    "alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calaulate $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$ for encryption.\n",
    "\n",
    "Let the message we are encrypting is a binary value e,g, $m = 1$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(306)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "\n",
    "beta %= q\n",
    "\n",
    "beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By *LWE assumption* $\\beta$ should look like a random value.\n",
    "Now the pair $(\\beta, \\vec{\\alpha})$ is our ciphertext.\n",
    "\n",
    "Let's decrypt the ciphertext above.\n",
    "\n",
    "As $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$, we can find $m + e = \\beta + \\left< \\vec{\\alpha}, \\vec{s} \\right>$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are very lucky, you might get the decrypted value.\n",
    "```\n",
    ">>> m_decrypted\n",
    "tensor(1)\n",
    "```\n",
    "But, if you run the code once, you will get other value.\n",
    "Note here that we get $m+e$ by decryption, *not the exact value* $m$.\n",
    "\n",
    "To make our message safe from the error, we can multiply certain *scaling factor* to our message.\n",
    "\n",
    "Here, let's multiply $q/4$, and encrypt/decrypt again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(513)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "# multiply scaling factor q/4 \n",
    "m *= q//4\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "beta %= q\n",
    "\n",
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a value near $m \\cdot q/4 = 512$.\n",
    "Division by $q/4$ and rounding will give us original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescale the message\n",
    "m_decrypted = m_decrypted.to(torch.float)\n",
    "m_decrypted /= q/4.\n",
    "m_decrypted = torch.round(m_decrypted)\n",
    "m_decrypted.to(torch.int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decryption is successful!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### LWE encryption function\n",
    "\n",
    "The LWE ciphertext is a pair $(\\beta, \\vec{\\alpha})$.\n",
    "\n",
    "We define the encryptor as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(740),\n",
       " tensor([ 492,  208, 1332,  531,  109, 1803,  186,  413, 1289,  558, 1651,  247,\n",
       "          937,  604,  241, 1369, 1096,  564, 1334, 1842, 2027,  453, 1886,   36,\n",
       "           50, 1663, 1056,  732,  597,  956,  534, 1227,  196, 1658, 1458, 1103,\n",
       "         2037, 1770, 1727, 1793, 1961, 1666,  680, 1342,  727, 1326,  436, 1491,\n",
       "         1825, 1130, 1636,   54,  855, 1983, 1251,  453,  622,  377, 1824,  477,\n",
       "          593, 1678,  830,  219,  178, 1878, 1138,  203,  400,  721, 1214, 1665,\n",
       "         1145,   41, 1101, 1345, 1664, 1892, 1854,  480,  380,  375,  619,  835,\n",
       "          222, 1899,  151, 1032, 1231,  496, 1769,  660, 1116, 1189,  396, 1641,\n",
       "          656, 1979,  934, 1460,  996,  452,  766, 1628, 1663, 1096,  293,  960,\n",
       "         1249,  826,  296,  801, 1475, 1663, 1712,  542, 1136, 1362, 1943,  815,\n",
       "          576, 1279, 1523,   55, 1329, 1907,  340, 1667,  916,  578,  949,  427,\n",
       "         1445,  965, 1145, 1831,  368, 1229, 1859,  918,  546,  506, 1108, 2027,\n",
       "          776,  302, 1163,  850, 1969,  519, 1440,  128,  364,  184, 1968,  161,\n",
       "          397,  184,  556, 1832, 2007, 1171,   95,  412,  488,  418, 1721,  584,\n",
       "         1113, 1853, 1010,  640, 1277, 1866,  563, 1382,  604,  181, 1904, 1312,\n",
       "         1496,  649,   34,  368,  978,  223, 1323,  967, 1386, 1017, 1703,  569,\n",
       "          837, 1316,  134,  438, 1167, 1075,  628,  855,    8, 1405,  294, 1900,\n",
       "          215,  571, 1726, 1648,   13,  513,  843,  479, 1804, 2001, 1825,  736,\n",
       "          492,  477,  671, 1292, 1491,  671, 1160,  351, 1685, 1275, 1024, 1630,\n",
       "          884,  326, 1382, 1807, 1067, 1000,  304, 1889, 1908, 1083,  884,  454,\n",
       "         1090, 1829, 2030, 1560, 1965,  361, 1912, 1293, 1014,  106,   24,   89,\n",
       "         1696,  299,  195, 1734, 1349,  561,  799,  952,  845,  526,  497, 1706,\n",
       "         2029, 1985, 1909,  162,  524,  812,  876, 1278, 1482,  268,   53,  410,\n",
       "          544, 1822, 1646,  666, 1789,  849,  481,  866, 1569,   28,  331, 1307,\n",
       "          826, 1327,   38, 1238, 1294,  516, 1732, 1506, 1284, 1933, 1019,  883,\n",
       "           99, 1060, 1051, 1626,   64, 1531,  191, 1687, 1423,  574, 1066, 1441,\n",
       "         1545, 1895,  375,  617,  254,   62,  489, 1800,  951,  789,   50,  201,\n",
       "          229,  640, 1619, 1074, 1031,  970, 1984, 1708, 1353,  532, 1384, 1023,\n",
       "          364, 1646,  336,  290, 1877,  773, 1373, 1683, 1904,  519,  879, 1968,\n",
       "          298, 1138,  492, 1409, 1119,  557, 1186, 1090, 1406, 1450, 1308, 1111,\n",
       "          555, 1653, 1081,   22,  445, 1679, 1955,   49,  701,  106, 1681, 2002,\n",
       "         1573, 1821, 1100, 1953,  786,  143,   44,  692, 1359, 1806, 1726,  887,\n",
       "         1065, 1469, 1216, 1516,  271,  406,  989,   55, 1302,  826, 1481,   65,\n",
       "          353, 1718, 1025,  749,  806,  539,  781, 1627, 1483, 1815,  706,  970,\n",
       "          278, 1215,  178, 1038, 1059, 2000,  998, 1038,  299, 1687,   91, 1795,\n",
       "           44, 1703, 1650,  548,   32,  783, 1149, 1802,  340,  832, 1138,  591,\n",
       "          528,    9, 1651,  508, 2035,  827,  353,  132, 1075,  438, 1401,  627,\n",
       "         1576, 1067, 2026,  712, 1350, 1664,  746,  795, 1511, 1605,  167,  882,\n",
       "         1491,  423, 1819, 1218, 1111,  751, 1231,   28, 1330, 1665,  440,  106,\n",
       "          983, 1620, 1251,  279, 1262, 1227,  151, 1063, 1869,  892,  734, 1399,\n",
       "          373, 1434, 1525, 1178,  883, 1908, 1013, 1776, 1644, 1738,  309, 1786,\n",
       "          595, 1422,  410,  873, 1248,   85,  287, 1789, 1022, 1007,  867,  702,\n",
       "         1211,  159,  299, 1130,  755, 2029,    7,  412]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encryptLWE(message, dim, modulus, key):\n",
    "    alpha = uniform(dim, modulus)\n",
    "\n",
    "    beta = message * modulus//4 - torch.dot(alpha, key)\n",
    "    e = errgen(stddev)\n",
    "    beta += e\n",
    "    beta %= modulus\n",
    "\n",
    "    return (beta, alpha)\n",
    "\n",
    "\n",
    "ct = encryptLWE(1, n, q, s)\n",
    "ct    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWE decryption function\n",
    "\n",
    "We can also define decryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decryptLWE(ct, key, modulus):\n",
    "    beta, alpha = ct\n",
    "    m_dec = beta + torch.dot(alpha, key)\n",
    "    m_dec %= modulus\n",
    "\n",
    "    m_dec = m_dec.to(torch.float)\n",
    "    m_dec /= modulus/4.\n",
    "    m_dec = torch.round(m_dec)\n",
    "    return m_dec.to(torch.int)\n",
    "\n",
    "decryptLWE(ct, s, q)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. RLWE encryption\n",
    "\n",
    "RLWE (a.k.a Ring-LWE) is a ring variant of LWE. The encrypted message in RLWE is a polynomial (with $N$ integer coefficients) rather than an integer.\n",
    "\n",
    "RLWE is more efficient, and easily define multiplication.\n",
    "\n",
    "In FHEW-like HE, we use the fact the the encrypted message is a *polynomial* and do computation on the exponent of $X$.\n",
    "\n",
    "### Integer ring\n",
    "\n",
    "As we use ring structure for RLWE, we define some important notation here. Please find detailed explanation in other papers. \n",
    "\n",
    "#### Polynomial\n",
    "\n",
    "We define set of polynomial $\\mathbb{Z}[X] = \\{\\sum_i a_i X^i : a_i \\in \\mathbb{Z}\\}$.\n",
    "Polynomial is denoted by bold characters e.g., $\\boldsymbol{a}$.\n",
    "\n",
    "#### Polynomial ring\n",
    "\n",
    "Usually, we set $N$ a power-of-two for efficiency.\n",
    "$\\mathcal{R} = \\mathbb{Z}[X]/\\left< X^N+1 \\right>$ denotes polynomial ring, and $\\phi_{2N}(X) = X^N+1$ is also called as a $2N$-th primitive polynomial.\n",
    "\n",
    "$\\mathcal{R}$ is a set of polynomial whose degree is less than $N$, and we can define addition and multiplication here.\n",
    " \n",
    "By multiplying two polynomials, the degree of product can be greater than or equal to $N$, in that case, we divide the product by $X^N + 1$ and use only the remainder.\n",
    "\n",
    "We define $\\mathcal{R}_Q = \\mathcal{R}/Q\\mathcal{R}$ as integer ring whose coefficients are modulus $Q$.\n",
    "\n",
    "$Q$ is selected as a prime number and $Q \\equiv 1 \\pmod{2N}$ for efficiency of NTT.\n",
    "\n",
    "However, for bervity, we just use 2**27.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**10\n",
    "Q = 2**27"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### implementation of integer polynomial\n",
    "\n",
    "We use torch tensor to represent a polynomial in $\\mathcal{R}_Q$ of length $N$, where its i-th element is coefficient of $X^i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35215391, 18094837, 47150314,  ..., 51357027, 97790261, 85164842])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random polynomial\n",
    "a = uniform(N, Q)\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLWE encryption and decryption\n",
    "We define RLWE encryption as follows\n",
    "$$\\textsf{RLWE}_{N,Q,\\boldsymbol{z}} = (\\boldsymbol{b}, \\boldsymbol{a}) \\in \\mathcal{R}_Q^2,$$\n",
    "where $\\boldsymbol{b} = \\boldsymbol{m} + \\boldsymbol{e} - \\boldsymbol{a} \\cdot \\boldsymbol{z}$.\n",
    "Here, $\\boldsymbol{a}$ is sampled unifromly from $\\mathcal{R}_Q$ and $\\boldsymbol{z} \\rightarrow \\chi_{key}$ is key, and $\\boldsymbol{e} \\rightarrow \\chi_{err}$ is added noise for security, similar to LWE.\n",
    "\n",
    "#### Why RLWE is more efficient than LWE?\n",
    "In HE, there are many reasons why RLWE is more efficient than LWE, but you can find the following if you see the equation carefully.\n",
    "\n",
    "An LWE ciphertext is composed of $n+1$ integers but encrypts only one value.\n",
    "However, RLWE ciphertext is compose of $2$ polynomials (=$2N$ integers), but encrypts $1$ polynomial (=$N$ integer coefficients)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's code\n",
    "We need a secret key $\\boldsymbol{z}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 1, 0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = keygen(N)\n",
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let our message $(1,0,0,0, ....)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.zeros(N).to(torch.int)\n",
    "m[0] = 1\n",
    "m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For encryption, we need $\\boldsymbol{a}$ and $\\boldsymbol{e}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([108202011,  80816390, 119055355,  ...,  60346144,  90332141,\n",
       "        127142132])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = uniform(N, Q)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  0,  ...,  4, -1,  1], dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def errpolygen(dim, stddev):\n",
    "    e = torch.round(stddev*torch.randn(dim))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "e = errpolygen(N, stddev)\n",
    "e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need multiplication between polynomials $\\boldsymbol{a}$ and $\\boldsymbol{z}$ for encryption.\n",
    "\n",
    "Let the $i$-th coefficient of $\\boldsymbol{a} \\cdot \\boldsymbol{z}$ be $(\\boldsymbol{a} \\cdot \\boldsymbol{z})_i$.\n",
    "\n",
    "Then, we have $ (\\boldsymbol{a} \\cdot \\boldsymbol{z})_i = \\sum_{j \\le i} a_j \\cdot z_{i-j} + \\sum_{j > i} - a_j \\cdot z_{N + i-j} $.\n",
    "\n",
    "The negative terms from the fact that $X^N = -1$ as we divide polynomials by $X^N+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polymult(a, b, dim, modulus):\n",
    "    res = torch.zeros(dim).to(torch.int)\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i >= j:\n",
    "                res[i] += a[j]*b[i-j]\n",
    "                res[i] %= modulus\n",
    "            else:\n",
    "                res[i] += modulus - a[j]*b[i-j] # Q - x mod Q = -x\n",
    "                res[i] %= modulus\n",
    "\n",
    "    res %= modulus\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make encrypt $\\boldsymbol{m}$.\n",
    "\n",
    "In LWE we used scaling factor $q/4$, but let me use 256 here for fun (large enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([103170463, 102521568,  24789522,  ..., 100989651,  17044564,\n",
       "         69928782], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = e - polymult(a, z, N, Q)\n",
    "b += (m * 2**8)\n",
    "b %= Q\n",
    "b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note polymult is **super slow** - in my M2-pro, it took 10.1s.\n",
    "\n",
    "The time complexity is $O(n^2)$\n",
    "\n",
    "We will handle this later (using NTT).\n",
    "\n",
    "The pair $(\\boldsymbol{b}, \\boldsymbol{a})$ is our RLWE encryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([103170463, 102521568,  24789522,  ..., 100989651,  17044564,\n",
       "          69928782], dtype=torch.int32),\n",
       " tensor([108202011,  80816390, 119055355,  ...,  60346144,  90332141,\n",
       "         127142132]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = (b, a)\n",
    "ct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decrypt the ciphertext using\n",
    "\n",
    "$ \\boldsymbol{m} + \\boldsymbol{e}  = \\boldsymbol{b} + \\boldsymbol{a}\\cdot \\boldsymbol{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([      256,         1,         0,  ...,         4, 134217727,\n",
       "                1], dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, a = ct\n",
    "\n",
    "m_decrypted = b + polymult(a, z, N, Q)\n",
    "m_decrypted %= Q\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values less than 0 will look like very big, e.g. -1 = 2**27 -1. \n",
    "Normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([256,   1,   0,  ...,   4,  -1,   1], dtype=torch.int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(N):\n",
    "    if m_decrypted[i] > Q//2:\n",
    "        m_decrypted[i] -= Q\n",
    "\n",
    "m_decrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_decrypted = m_decrypted.to(torch.float) / float(2**8)\n",
    "torch.round(m_decrypted)\n",
    "m_decrypted = torch.round(m_decrypted).to(torch.int)\n",
    "\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned LWE/RLWE encryption and decryption in this chapter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
