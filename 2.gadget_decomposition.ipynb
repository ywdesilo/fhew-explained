{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gadget decomposition\n",
    "\n",
    "**GOAL:** for a given polynomial $\\boldsymbol{a}$ and an encryption $\\textsf{RLWE}(m)$ (or something similar to it), find $\\textsf{RLWE}(\\boldsymbol{a}\\cdot \\boldsymbol{m})$, with small noise.\n",
    "\n",
    "Gadget decomposition and $RLWE'$ technique allows us to multiply ciphertext to a *large* constant with *small* noise increment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLWE' is usually used for the key switching. We make an example of key switching from s1 to s2 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from previous lecturenote\n",
    "import torch\n",
    "import math\n",
    "\n",
    "stddev = 3.2\n",
    "logQ = 27\n",
    "\n",
    "N = 2**10\n",
    "Q = 2**logQ\n",
    "\n",
    "\n",
    "def keygen(dim):\n",
    "    return torch.randint(2, size = (dim,))\n",
    "\n",
    "def errgen(stddev):\n",
    "    e = torch.round(stddev*torch.randn(1))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "def errgen(stddev, N):\n",
    "    e = torch.round(stddev*torch.randn(N))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "def uniform(dim, modulus):\n",
    "    return torch.randint(modulus, size = (dim,))\n",
    "\n",
    "def polymult(a, b, dim, modulus):\n",
    "    res = torch.zeros(dim).to(torch.int)\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i >= j:\n",
    "                res[i] += a[j]*b[i-j]\n",
    "                res[i] %= modulus\n",
    "            else:\n",
    "                res[i] -= a[j]*b[i-j] # Q - x mod Q = -x\n",
    "                res[i] %= modulus\n",
    "\n",
    "    res %= modulus\n",
    "    return res\n",
    "\n",
    "root_powers = torch.arange(N//2).to(torch.complex128)\n",
    "root_powers = torch.exp((1j*math.pi/N)*root_powers)\n",
    "\n",
    "root_powers_inv = torch.arange(0,-N//2,-1).to(torch.complex128)\n",
    "root_powers_inv = torch.exp((1j*math.pi/N)*root_powers_inv)\n",
    "\n",
    "def negacyclic_fft(a, N, Q):\n",
    "    acomplex = a.to(torch.complex128)\n",
    "\n",
    "    a_precomp = (acomplex[...,:N//2] + 1j * acomplex[..., N//2:]) * root_powers\n",
    "\n",
    "    return torch.fft.fft(a_precomp)\n",
    "\n",
    "def negacyclic_ifft(A, N, Q):\n",
    "    b = torch.fft.ifft(A)\n",
    "    b *= root_powers_inv\n",
    "\n",
    "    a = torch.cat((b.real, b.imag), dim=-1)\n",
    "\n",
    "    aint = a.to(torch.int32)\n",
    "    # only when Q is a power-of-two\n",
    "    aint &= Q-1\n",
    "\n",
    "    return aint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two keys\n",
    "s1 = keygen(N)\n",
    "s2 = keygen(N)\n",
    "\n",
    "s1fft = negacyclic_fft(s1, N, Q)\n",
    "s2fft = negacyclic_fft(s2, N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an RLWE encryption of message\n",
    "def encrypt_to_fft(m, sfft):\n",
    "    ct = torch.stack([errgen(stddev, N), uniform(N, Q)])\n",
    "    ctfft = negacyclic_fft(ct, N, Q)\n",
    "\n",
    "    ctfft[0] += -ctfft[1]*sfft + negacyclic_fft(m, N, Q)\n",
    "\n",
    "    return ctfft\n",
    "\n",
    "def decrypt_from_fft(ctfft, sfft):\n",
    "    return negacyclic_ifft(ctfft[0] + ctfft[1]*sfft, N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1000000,       0,       0,  ...,       0,       0,       0],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.zeros((N), dtype=torch.int32)\n",
    "m[0] = 1000000\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1000003,         2, 134217726,  ...,         5,         6,\n",
       "                1], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctfft = encrypt_to_fft(m, s1fft)\n",
    "mdec = decrypt_from_fft(ctfft, s1fft)\n",
    "mdec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot decrypt mdec with s2, as the secret key is different. It should look like a random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 13737172, 127554412,  96457619,  ...,  49456183,  19749892,\n",
       "         37101897], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdec_wrong = decrypt_from_fft(ctfft, s2fft)\n",
    "mdec_wrong"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transform ct as a ciphertext of the same message but with different key s2 *without decryption*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Gadget decomposition\n",
    "\n",
    "We define *gadget decomposition* $h$ corresponding to gadget vector $\\vec{g} = (g_0, g_1, \\dots, g_{d-1})$ as follows.\n",
    "$$\n",
    "h: \\mathbb{Z} \\longmapsto \\mathbb{Z}^d\n",
    "$$\n",
    "$$\n",
    "||h(a)|| < B, \\left< h(a), \\vec{g}\\right> = a,\n",
    "$$\n",
    "where B is a upper bound.\n",
    "\n",
    "Also, we can naturally extend it to $\\mathcal{R}$ and $\\mathbb{Z}^n$.\n",
    "\n",
    "For example, a number $77 = 0\\text{b}01001101$ can be decomposed to $(0,1,0,0,1,1,0,1)$ when $\\vec{g} = (2^7, 2^6, \\dots, 1)$ and $B=2$\n",
    "\n",
    "Otherwise, it can also be decomposed to $(1,0,3,1)$ when $\\vec{g} = (4^3, 4^2, 4, 1)$, and $B=4$ here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can do a *approximate gadget decomposition* where $\\left< h(a), \\vec{g}\\right> \\approx a$, i.e., $\\left< h(a), \\vec{g}\\right>$ is not exactly the same but similar to $a$.\n",
    "\n",
    "We first set d and B satisfying $B^{d} \\le Q \\le B^{d+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use B-ary decomposition, i.e., cut digits by logB bits\n",
    "d = 3\n",
    "logB = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20],\n",
       "        [13],\n",
       "        [ 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomp_shift = logQ - logB*torch.arange(1,d+1).unsqueeze(dim = 1)\n",
    "decomp_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133169152],\n",
       "        [  1040384],\n",
       "        [     8128]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((1<<logB) -1) is 0b1111111 (len of 1 is logB)\n",
    "decomp_mask = ((1<<logB) -1) << decomp_shift\n",
    "decomp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1048576],\n",
       "        [   8192],\n",
       "        [     64]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gvector = 1<<decomp_shift\n",
    "gvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose(a):\n",
    "    da = a.unsqueeze(dim = a.dim()-1)\n",
    "    \n",
    "    newdim = list(da.size())\n",
    "    newdim[-2] = d\n",
    "    \n",
    "    da = da.expand(newdim)\n",
    "\n",
    "    return (da & decomp_mask) >> decomp_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 65261118, 125653224,   7230529,  ...,  82140274,  67572063,\n",
       "         19397401])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = uniform(N, Q)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = decompose(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 65261056, 125653184,   7230528,  ...,  82140224,  67572032,\n",
       "         19397376])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#composition is inner product, see it is similar to a\n",
    "torch.sum(da * gvector, dim = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend it to a ciphertext too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4486380,   5226831,   3046802,  ...,  86454026,  62226672,\n",
       "          61140349],\n",
       "        [ 93497099,   7935531, 116776838,  ...,  51850894,  41694239,\n",
       "          50890776]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctfft = encrypt_to_fft(m, s1fft)\n",
    "ct = negacyclic_ifft(ctfft, N, Q)\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4486336,   5226816,   3046784,  ...,  86454016,  62226624,\n",
       "          61140288],\n",
       "        [ 93497088,   7935488, 116776832,  ...,  51850880,  41694208,\n",
       "          50890752]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(decompose(ct) * gvector, dim = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
