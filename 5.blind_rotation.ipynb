{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Blind rotation\n",
    "A blind rotation procdeure is the core of FHEW bootstrapping and other many application of HE supporting non-arithmetic (not $\\times$ or +) operations.\n",
    "\n",
    "It is defined as follows.\n",
    "\n",
    "**Definition (blind rotation)**\n",
    "- Input: LWE ciphertext $(\\beta, \\vec{\\alpha})$, blind rotation keys $brk$ under secret $\\boldsymbol{z}$, and public polynomial $f$, where $\\beta + \\left< \\vec{\\alpha}, \\vec{s}\\right> = u$\n",
    "- Output: $RLWE_{\\boldsymbol{z}}(f\\cdot X^u)$\n",
    "\n",
    "It is compose of following three steps.\n",
    "1. Make a encryption of $f\\cdot X^{\\beta}$, $ACC$. It can easily be done $ACC = (f\\cdot X^{\\beta}, 0)$.\n",
    "2. Accumulation: homomophically multiply $X^{\\alpha_i s_i}$ to $ACC$ and update it.\n",
    "3. After accumulation, we get $RLWE_{\\boldsymbol{z}}(f\\cdot X^{\\beta + \\left< \\vec{\\alpha}, \\vec{s}\\right>}) = RLWE_{\\boldsymbol{z}}(f\\cdot X^u)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from previous lecturenote\n",
    "import torch\n",
    "import math\n",
    "\n",
    "stddev = 3.2\n",
    "logQ = 27\n",
    "\n",
    "N = 2**10\n",
    "Q = 2**logQ\n",
    "\n",
    "n = 512\n",
    "q = 2*N\n",
    "\n",
    "def keygen(dim):\n",
    "    return torch.randint(2, size = (dim,), dtype=torch.int32)\n",
    "\n",
    "def errgen(stddev):\n",
    "    e = torch.round(stddev*torch.randn(1))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "def errgen(stddev, N):\n",
    "    e = torch.round(stddev*torch.randn(N))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "\n",
    "def uniform(dim, modulus):\n",
    "    return torch.randint(modulus, size = (dim,), dtype=torch.int32)\n",
    "\n",
    "def polymult(a, b, dim, modulus):\n",
    "    res = torch.zeros(dim).to(torch.int)\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            if i >= j:\n",
    "                res[i] += a[j]*b[i-j]\n",
    "                res[i] %= modulus\n",
    "            else:\n",
    "                res[i] -= a[j]*b[i-j] # Q - x mod Q = -x\n",
    "                res[i] %= modulus\n",
    "\n",
    "    res %= modulus\n",
    "    return res\n",
    "\n",
    "root_powers = torch.arange(N//2).to(torch.complex128)\n",
    "root_powers = torch.exp((1j*math.pi/N)*root_powers)\n",
    "\n",
    "root_powers_inv = torch.arange(0,-N//2,-1).to(torch.complex128)\n",
    "root_powers_inv = torch.exp((1j*math.pi/N)*root_powers_inv)\n",
    "\n",
    "def negacyclic_fft(a, N, Q):\n",
    "    acomplex = a.to(torch.complex128)\n",
    "\n",
    "    a_precomp = (acomplex[...,:N//2] + 1j * acomplex[..., N//2:]) * root_powers\n",
    "\n",
    "    return torch.fft.fft(a_precomp)\n",
    "\n",
    "def negacyclic_ifft(A, N, Q):\n",
    "    b = torch.fft.ifft(A)\n",
    "    b *= root_powers_inv\n",
    "\n",
    "    a = torch.cat((b.real, b.imag), dim=-1)\n",
    "    # Rounding should be more accurate\n",
    "    # a += 0.5\n",
    "\n",
    "    aint = a.to(torch.int32)\n",
    "    # only when Q is a power-of-two\n",
    "    aint &= Q-1\n",
    "\n",
    "    return aint\n",
    "\n",
    "# make an RLWE encryption of message\n",
    "def encrypt_to_fft(m, sfft):\n",
    "    ct = torch.stack([errgen(stddev, N), uniform(N, Q)])\n",
    "    ctfft = negacyclic_fft(ct, N, Q)\n",
    "\n",
    "    ctfft[0] += -ctfft[1]*sfft + negacyclic_fft(m, N, Q)\n",
    "\n",
    "    return ctfft\n",
    "\n",
    "def normalize(v, logQ):\n",
    "    # same as follows but no branch\n",
    "    \"\"\"\n",
    "    if v > Q//2:\n",
    "        v -= Q\n",
    "    \"\"\"\n",
    "    # vmod Q when Q is a power-of-two\n",
    "    Q = (1 << logQ)\n",
    "    v &= Q-1\n",
    "    # get msb\n",
    "    msb = (v & Q//2) >> (logQ - 1)\n",
    "    v -= (Q) * msb\n",
    "    return v\n",
    "\n",
    "def decrypt_from_fft(ctfft, sfft):\n",
    "    assert len(ctfft.size()) == 2\n",
    "    # normalization is optional\n",
    "    return normalize(negacyclic_ifft(ctfft[0] + ctfft[1]*sfft, N, Q), logQ)\n",
    "    # return negacyclic_ifft(ctfft[0] + ctfft[1]*sfft, N, Q)\n",
    "\n",
    "# we will use B-ary decomposition, i.e., cut digits by logB bits\n",
    "d = 4\n",
    "logB = 6\n",
    "\n",
    "assert d * logB < logQ\n",
    "\n",
    "decomp_shift = logQ - logB*torch.arange(d,0,-1).view(d,1)\n",
    "mask = (1 << logB) - 1\n",
    "\n",
    "gvector = 1 << decomp_shift\n",
    "\n",
    "def decompose(a):\n",
    "    \n",
    "    assert len(a.size()) <= 2\n",
    "    # for RLWE'\n",
    "    if len(a.size()) == 1:\n",
    "        res = (a.unsqueeze(0) >> decomp_shift.view(d, 1)) & mask\n",
    "        return res\n",
    "    # for RGSW\n",
    "    elif len(a.size()) == 2:\n",
    "        res = (a.unsqueeze(0) >> decomp_shift.view(d, 1, 1)) & mask\n",
    "        return res\n",
    "\n",
    "msbmask = 0\n",
    "for i in decomp_shift:\n",
    "    msbmask += (1<<(i+logB-1))\n",
    "\n",
    "bin(msbmask)[2:]\n",
    "\n",
    "# about twice heavier than unsigned decomposition\n",
    "# it returns value -B/2 <= * <= B/2, not < B/2, but okay\n",
    "def signed_decompose(a):\n",
    "    # carry\n",
    "    da = decompose(a + (a & msbmask))\n",
    "    # -B\n",
    "    da -= decompose((a & msbmask))\n",
    "    return da\n",
    "\n",
    "def encrypt_rgsw_fft(z, skfft):\n",
    "    # RGSW has a dimension of d, 2, 2, N\n",
    "    rgsw = torch.zeros(d, 2, 2, N, dtype=torch.int32)\n",
    "\n",
    "    # generate the 'a' part\n",
    "    # INSECURE: to be fixed later\n",
    "    rgsw[:, :, 1, :] = torch.randint(Q, size = (d, 2 , N), dtype= torch.int32)\n",
    "\n",
    "    # add error on b\n",
    "    # INSECURE: to be fixed later\n",
    "    rgsw[:, :, 0, :] = torch.round(stddev * torch.randn(size = (d, 2, N)))\n",
    "\n",
    "    # following is equal to rgsw %= Q, but a faster version\n",
    "    rgsw &= (Q-1)\n",
    "    rgsw = normalize(rgsw, logQ)\n",
    "\n",
    "    # do fft for easy a*s\n",
    "    rgswfft = negacyclic_fft(rgsw, N, Q)\n",
    "\n",
    "    # now b = -a*sk + e\n",
    "    rgswfft[:, :, 0, :] -= rgswfft[:, :, 1, :] * skfft.view(1, 1, N//2)\n",
    "\n",
    "    # encrypt (z, z*sk) multiplied by g\n",
    "    gzfft = negacyclic_fft(gvector * z, N, Q)\n",
    "    rgswfft[:, 0, 0, :] += gzfft\n",
    "    rgswfft[:, 1, 1, :] += gzfft\n",
    "\n",
    "    return rgswfft\n",
    "\n",
    "def rgswmult(ctfft, rgswfft):\n",
    "    ct = negacyclic_ifft(ctfft, N, Q)\n",
    "    dct = signed_decompose(ct)\n",
    "    multfft = negacyclic_fft(dct, N, Q).view(d, 2, 1, N//2) * rgswfft\n",
    "    \n",
    "    return torch.sum(multfft, dim = (0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encryptLWE(message, dim, modulus, key):\n",
    "    ct = uniform(dim + 1, modulus)\n",
    "\n",
    "    ct[0] = 0\n",
    "\n",
    "    ct[0] = message * modulus//4 - torch.dot(ct[1:], key)\n",
    "    ct[0] += errgen(stddev, 1)\n",
    "    ct &= modulus -1\n",
    "\n",
    "    return ct\n",
    "\n",
    "def decryptLWE(ct, sk, modulus):\n",
    "    m_dec = torch.dot(ct, torch.cat((torch.ones(1, dtype=torch.int32), sk)))\n",
    "    m_dec %= modulus\n",
    "\n",
    "    m_dec = m_dec.to(torch.float)\n",
    "    m_dec /= modulus/4.\n",
    "    m_dec = torch.round(m_dec)\n",
    "    return m_dec.to(torch.int)%4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(ctRLWE):\n",
    "    beta = ctRLWE[0][0]\n",
    "\n",
    "    alpha = ctRLWE[1][:]\n",
    "    alpha[1:] = -alpha[1:].flip(dims = [0])\n",
    "\n",
    "    return torch.cat((beta.unsqueeze(0), alpha))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 CMux gate\n",
    "\n",
    "In each iteration, we need to multiply $RGSW(X^{\\alpha_i s_i})$, where $\\alpha_i$ is public and $s_i$ is given in encrypted from.\n",
    "The variants: AP/DM, GINX/CGGI, and LMKCDEY differ in generation of encrypted $X^{\\alpha_i s_i}$.\n",
    "\n",
    "In GINX variant blind rotation for *binary secrets*, we use the following CMux gate as subprocess.\n",
    "$$\n",
    "X^{\\alpha_i s_i} = (1-s_i) + X^{\\alpha_i} \\cdot s_i\n",
    "$$\n",
    "There are only two possible cases = $s_i$ is $0$ or $1$ - thus, we can easily see that the equation holds in both cases.\n",
    "\n",
    "1. $s_i = 0$\n",
    "$$\n",
    "X^{\\alpha_i s_i} = (1-s_i) + X^{\\alpha_i} \\cdot s_i \\\\\n",
    "    = (1-0) + 0\\\\\n",
    "    = X^0 \n",
    "$$\n",
    "2. $s_i = 1$\n",
    "$$\n",
    "X^{\\alpha_i s_i} = (1-1) + X^{\\alpha_i} \\cdot 1 \\\\\n",
    "    = (1-1) +  X^{\\alpha_i}\\\\\n",
    "    = X^{\\alpha_i}\n",
    "$$\n",
    "\n",
    "Hence, we calculate the following in each iteration\n",
    "$$\n",
    "ACC \\leftarrow ACC \\circledast ((1-RGSW(s_i)) + X^{\\alpha_i} \\cdot RGSW(s_i)),\n",
    "$$\n",
    "which can be computed efficiently with\n",
    "$$\n",
    "ACC \\leftarrow ACC  + (X^{\\alpha_i} - 1 ) \\cdot ACC\\circledast RGSW(s_i).\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the GINX accumulation in the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1\n",
    "skN = keygen(N)\n",
    "skNfft = negacyclic_fft(skN, N, Q)\n",
    "\n",
    "sPoly = torch.zeros([N], dtype=torch.int32)\n",
    "sPoly[0] = s\n",
    "\n",
    "rgswKey = encrypt_rgsw_fft(sPoly, skNfft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      0, 1000000, 2000000,  ..., 1000000, 2000000, 3000000],\n",
       "        [      0,       0,       0,  ...,       0,       0,       0]],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ACC is transparent encryption of (0,1,2, ..., N)\n",
    "ACC = torch.zeros([2,N], dtype=torch.int32)\n",
    "for i in range(N):\n",
    "    ACC[0][i] = (i%10) * 1000000\n",
    "\n",
    "ACCfft = negacyclic_fft(ACC, N, Q)\n",
    "\n",
    "ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3\n",
    "alphaPoly = torch.zeros([N], dtype=torch.int32)\n",
    "alphaPoly[0] = -1\n",
    "alphaPoly[alpha] = 1\n",
    "\n",
    "alphaPolyfft = negacyclic_fft(alphaPoly, N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulation\n",
    "ACCfft += alphaPolyfft * rgswmult(ACCfft, rgswKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0021e+00, -2.0053e+00, -2.9950e+00, -2.5510e-03,  1.0061e+00,\n",
       "         1.9955e+00,  3.0043e+00,  3.9975e+00,  5.0053e+00,  5.9951e+00])"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar to (*, *, *, 0, 1, 2, 3, 4, ...) = m * X^{alpha s}\n",
    "decrypt_from_fft(ACCfft, skNfft)[:10]/1000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do blind rotation\n",
    "\n",
    "Generate blind rotation keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = keygen(n)\n",
    "\n",
    "zero_poly = torch.zeros([N], dtype=torch.int32)\n",
    "\n",
    "one_poly = torch.zeros([N], dtype=torch.int32)\n",
    "one_poly[0] = 1\n",
    "\n",
    "brk = [None]*n\n",
    "for i in range(n):\n",
    "    if sk[i] == 0:\n",
    "        brk[i] = encrypt_rgsw_fft(zero_poly, skNfft)\n",
    "    else:\n",
    "        brk[i] = encrypt_rgsw_fft(one_poly, skNfft)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precompute alphapolys in FFT form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphapoly = []\n",
    "for i in range(q):\n",
    "    poly = torch.zeros([N], dtype=torch.int32)\n",
    "    poly[0] = -1\n",
    "    if i < N:\n",
    "        poly[i] += 1\n",
    "    else:\n",
    "        poly[i-N] += -1\n",
    "    alphapoly.append(negacyclic_fft(poly, N, Q))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate $f$ for NAND gate.\n",
    "$[-q/8, 3q/8)$ is mapped to $q/8$, and $[3q/8, 7q/8)$ is mapped to $-q/8$\n",
    "\n",
    "NOTE: $f$ depends on the binary gate we want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nand_map(i):\n",
    "    i += 2*N \n",
    "    i %= 2*N\n",
    "    if 3*(q>>3) <= i < 7*(q>>3): # i \\in [3q/8, 7q/8)\n",
    "        return -(Q >> 3)\n",
    "    else: # i \\in [-q/8, 3q/8)\n",
    "        return Q >> 3 \n",
    "\n",
    "f_nand = torch.zeros([N], dtype=torch.int32)\n",
    "\n",
    "for i in range(N):\n",
    "    f_nand[i] = nand_map(-i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two LWE encryptions are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = 0\n",
    "m1 = 0\n",
    "\n",
    "ct0 = encryptLWE(m0, n, q, sk)\n",
    "ct1 = encryptLWE(m1, n, q, sk)\n",
    "\n",
    "ctsum = (ct0 + ct1) & (q-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ctsum` encrypts `m0 + m1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.int32)"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decryptLWE(ctsum, sk, q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `ACC` $= RLWE(f \\cdot X^\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = torch.zeros([2,N], dtype=torch.int32)\n",
    "acc[0] = f_nand\n",
    "\n",
    "accfft = negacyclic_fft(acc, N, Q)\n",
    "\n",
    "beta = ctsum[0]\n",
    "xbeta = torch.zeros([N], dtype=torch.int32)\n",
    "\n",
    "if beta < N:\n",
    "    xbeta[beta] = 1\n",
    "else:\n",
    "    xbeta[beta - N] = -1\n",
    "\n",
    "accfft *= negacyclic_fft(xbeta, N, Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do blind rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 tensor(-9.0972e-06) tensor(7.1526e-06)\n",
      "1 1 tensor(-0.4995) tensor(0.4995)\n",
      "2 0 tensor(-0.4995) tensor(0.4995)\n",
      "3 1 tensor(-0.4987) tensor(0.4990)\n",
      "4 1 tensor(-0.4969) tensor(0.4997)\n",
      "5 0 tensor(-0.4969) tensor(0.4997)\n",
      "6 0 tensor(-0.4969) tensor(0.4997)\n",
      "7 0 tensor(-0.4969) tensor(0.4997)\n",
      "8 0 tensor(-0.4969) tensor(0.4997)\n",
      "9 1 tensor(-0.4998) tensor(0.4998)\n",
      "10 1 tensor(-0.4995) tensor(0.4989)\n",
      "11 0 tensor(-0.4995) tensor(0.4989)\n",
      "12 1 tensor(-0.4975) tensor(0.4994)\n",
      "13 1 tensor(-0.4998) tensor(0.4992)\n",
      "14 1 tensor(-0.4984) tensor(0.4993)\n",
      "15 1 tensor(-0.4987) tensor(0.4984)\n",
      "16 0 tensor(-0.4987) tensor(0.4984)\n",
      "17 1 tensor(-0.4986) tensor(0.4985)\n",
      "18 1 tensor(-0.4969) tensor(0.4994)\n",
      "19 0 tensor(-0.4969) tensor(0.4994)\n",
      "20 0 tensor(-0.4969) tensor(0.4994)\n",
      "21 1 tensor(-0.4995) tensor(0.4995)\n",
      "22 1 tensor(-0.4997) tensor(0.4997)\n",
      "23 1 tensor(-0.4973) tensor(0.4996)\n",
      "24 1 tensor(-0.4999) tensor(0.5000)\n",
      "25 0 tensor(-0.4999) tensor(0.5000)\n",
      "26 0 tensor(-0.4999) tensor(0.5000)\n",
      "27 1 tensor(-0.5000) tensor(0.4989)\n",
      "28 1 tensor(-0.4995) tensor(0.5000)\n",
      "29 1 tensor(-0.4999) tensor(0.4979)\n",
      "30 1 tensor(-0.5000) tensor(0.4980)\n",
      "31 0 tensor(-0.5000) tensor(0.4980)\n",
      "32 1 tensor(-0.4993) tensor(0.4996)\n",
      "33 1 tensor(-0.4995) tensor(0.4983)\n",
      "34 0 tensor(-0.4995) tensor(0.4983)\n",
      "35 1 tensor(-0.4996) tensor(0.4987)\n",
      "36 1 tensor(-0.4987) tensor(0.4993)\n",
      "37 0 tensor(-0.4987) tensor(0.4993)\n",
      "38 0 tensor(-0.4987) tensor(0.4993)\n",
      "39 1 tensor(-0.4975) tensor(0.4985)\n",
      "40 0 tensor(-0.4975) tensor(0.4985)\n",
      "41 0 tensor(-0.4975) tensor(0.4985)\n",
      "42 1 tensor(-0.4989) tensor(0.4997)\n",
      "43 1 tensor(-0.4986) tensor(0.5000)\n",
      "44 1 tensor(-0.4992) tensor(0.5000)\n",
      "45 0 tensor(-0.4992) tensor(0.5000)\n",
      "46 0 tensor(-0.5000) tensor(0.4971)\n",
      "47 1 tensor(-0.4994) tensor(0.4994)\n",
      "48 1 tensor(-0.4984) tensor(0.4978)\n",
      "49 1 tensor(-0.4981) tensor(0.4974)\n",
      "50 1 tensor(-0.4996) tensor(0.4995)\n",
      "51 0 tensor(-0.4996) tensor(0.4995)\n",
      "52 0 tensor(-0.4996) tensor(0.4995)\n",
      "53 1 tensor(-0.4997) tensor(0.4985)\n",
      "54 0 tensor(-0.4997) tensor(0.4985)\n",
      "55 1 tensor(-0.5000) tensor(0.4997)\n",
      "56 1 tensor(-0.4977) tensor(0.4987)\n",
      "57 0 tensor(-0.4977) tensor(0.4987)\n",
      "58 1 tensor(-0.4975) tensor(0.4997)\n",
      "59 1 tensor(-0.5000) tensor(0.4980)\n",
      "60 1 tensor(-0.4988) tensor(0.4988)\n",
      "61 0 tensor(-0.4988) tensor(0.4987)\n",
      "62 0 tensor(-0.4988) tensor(0.4987)\n",
      "63 0 tensor(-0.4988) tensor(0.4987)\n",
      "64 0 tensor(-0.4988) tensor(0.4987)\n",
      "65 0 tensor(-0.4988) tensor(0.4987)\n",
      "66 1 tensor(-0.4980) tensor(0.4991)\n",
      "67 1 tensor(-0.4965) tensor(0.4989)\n",
      "68 0 tensor(-0.4965) tensor(0.4989)\n",
      "69 0 tensor(-0.4965) tensor(0.4989)\n",
      "70 1 tensor(-0.4985) tensor(0.4995)\n",
      "71 0 tensor(-0.4985) tensor(0.4995)\n",
      "72 1 tensor(-0.4983) tensor(0.4975)\n",
      "73 1 tensor(-0.4972) tensor(0.4993)\n",
      "74 0 tensor(-0.4972) tensor(0.4993)\n",
      "75 0 tensor(-0.4972) tensor(0.4993)\n",
      "76 0 tensor(-0.4972) tensor(0.4993)\n",
      "77 1 tensor(-0.4996) tensor(0.4996)\n",
      "78 0 tensor(-0.4996) tensor(0.4996)\n",
      "79 0 tensor(-0.4996) tensor(0.4996)\n",
      "80 1 tensor(-0.4991) tensor(0.4990)\n",
      "81 0 tensor(-0.4991) tensor(0.4991)\n",
      "82 0 tensor(-0.4991) tensor(0.4991)\n",
      "83 0 tensor(-0.4991) tensor(0.4991)\n",
      "84 1 tensor(-0.4980) tensor(0.5000)\n",
      "85 0 tensor(-0.4980) tensor(0.5000)\n",
      "86 1 tensor(-0.4999) tensor(0.4992)\n",
      "87 1 tensor(-0.4979) tensor(0.4999)\n",
      "88 1 tensor(-0.4993) tensor(0.4976)\n",
      "89 1 tensor(-0.4993) tensor(0.4941)\n",
      "90 0 tensor(-0.4993) tensor(0.4941)\n",
      "91 0 tensor(-0.4993) tensor(0.4941)\n",
      "92 0 tensor(-0.4993) tensor(0.4941)\n",
      "93 1 tensor(-0.4983) tensor(0.4993)\n",
      "94 1 tensor(-0.4994) tensor(0.4998)\n",
      "95 0 tensor(-0.4994) tensor(0.4998)\n",
      "96 0 tensor(-0.4994) tensor(0.4998)\n",
      "97 0 tensor(-0.4994) tensor(0.4998)\n",
      "98 1 tensor(-0.4987) tensor(0.4984)\n",
      "99 1 tensor(-0.4987) tensor(0.4972)\n",
      "100 0 tensor(-0.4987) tensor(0.4972)\n",
      "101 1 tensor(-0.4999) tensor(0.4983)\n",
      "102 0 tensor(-0.4999) tensor(0.4983)\n",
      "103 0 tensor(-0.4999) tensor(0.4983)\n",
      "104 0 tensor(-0.4999) tensor(0.4983)\n",
      "105 0 tensor(-0.4999) tensor(0.4983)\n",
      "106 1 tensor(-0.4973) tensor(0.4976)\n",
      "107 0 tensor(-0.4973) tensor(0.4976)\n",
      "108 0 tensor(-0.4973) tensor(0.4976)\n",
      "109 0 tensor(-0.4973) tensor(0.4976)\n",
      "110 1 tensor(-0.4997) tensor(0.4973)\n",
      "111 1 tensor(-0.4982) tensor(0.4995)\n",
      "112 0 tensor(-0.4982) tensor(0.4995)\n",
      "113 1 tensor(-0.4974) tensor(0.4992)\n",
      "114 1 tensor(-0.5000) tensor(0.4988)\n",
      "115 0 tensor(-0.5000) tensor(0.4988)\n",
      "116 1 tensor(-0.4998) tensor(0.4985)\n",
      "117 0 tensor(-0.4998) tensor(0.4985)\n",
      "118 1 tensor(-0.4999) tensor(0.4987)\n",
      "119 0 tensor(-0.4999) tensor(0.4987)\n",
      "120 1 tensor(-0.4986) tensor(0.4961)\n",
      "121 1 tensor(-0.4980) tensor(0.4988)\n",
      "122 0 tensor(-0.4980) tensor(0.4988)\n",
      "123 0 tensor(-0.4980) tensor(0.4988)\n",
      "124 0 tensor(-0.4980) tensor(0.4988)\n",
      "125 0 tensor(-0.4980) tensor(0.4988)\n",
      "126 0 tensor(-0.4980) tensor(0.4988)\n",
      "127 1 tensor(-0.4991) tensor(0.4996)\n",
      "128 1 tensor(-0.4996) tensor(0.4996)\n",
      "129 0 tensor(-0.4996) tensor(0.4996)\n",
      "130 1 tensor(-0.4988) tensor(0.4991)\n",
      "131 0 tensor(-0.4988) tensor(0.4991)\n",
      "132 1 tensor(-0.4993) tensor(0.4997)\n",
      "133 0 tensor(-0.4993) tensor(0.4997)\n",
      "134 1 tensor(-0.4970) tensor(0.4985)\n",
      "135 1 tensor(-0.4987) tensor(0.4991)\n",
      "136 1 tensor(-0.4971) tensor(0.5000)\n",
      "137 1 tensor(-0.4999) tensor(0.4997)\n",
      "138 0 tensor(-0.4999) tensor(0.4997)\n",
      "139 1 tensor(-0.4982) tensor(0.4998)\n",
      "140 0 tensor(-0.4982) tensor(0.4998)\n",
      "141 1 tensor(-0.4992) tensor(0.4988)\n",
      "142 1 tensor(-0.4995) tensor(0.4955)\n",
      "143 0 tensor(-0.4995) tensor(0.4955)\n",
      "144 1 tensor(-0.4991) tensor(0.4998)\n",
      "145 1 tensor(-0.5000) tensor(0.4982)\n",
      "146 1 tensor(-0.4969) tensor(0.4922)\n",
      "147 1 tensor(-0.4998) tensor(0.4954)\n",
      "148 0 tensor(-0.4998) tensor(0.4954)\n",
      "149 0 tensor(-0.4998) tensor(0.4954)\n",
      "150 0 tensor(-0.4998) tensor(0.4954)\n",
      "151 0 tensor(-0.4998) tensor(0.4954)\n",
      "152 0 tensor(-0.4998) tensor(0.4954)\n",
      "153 1 tensor(-0.4993) tensor(0.4987)\n",
      "154 1 tensor(-0.4997) tensor(0.4995)\n",
      "155 0 tensor(-0.4997) tensor(0.4995)\n",
      "156 0 tensor(-0.4997) tensor(0.4995)\n",
      "157 1 tensor(-0.4991) tensor(0.4995)\n",
      "158 1 tensor(-0.4995) tensor(0.4991)\n",
      "159 0 tensor(-0.4995) tensor(0.4991)\n",
      "160 1 tensor(-0.4994) tensor(0.4999)\n",
      "161 1 tensor(-0.4998) tensor(0.4986)\n",
      "162 1 tensor(-0.4990) tensor(0.4981)\n",
      "163 0 tensor(-0.4990) tensor(0.4981)\n",
      "164 1 tensor(-0.4988) tensor(0.4991)\n",
      "165 1 tensor(-0.4973) tensor(0.4935)\n",
      "166 0 tensor(-0.4973) tensor(0.4935)\n",
      "167 0 tensor(-0.4973) tensor(0.4935)\n",
      "168 1 tensor(-0.4994) tensor(0.4965)\n",
      "169 0 tensor(-0.4994) tensor(0.4965)\n",
      "170 0 tensor(-0.4994) tensor(0.4965)\n",
      "171 0 tensor(-0.4994) tensor(0.4965)\n",
      "172 0 tensor(-0.4994) tensor(0.4965)\n",
      "173 1 tensor(-0.4984) tensor(0.4978)\n",
      "174 0 tensor(-0.4984) tensor(0.4978)\n",
      "175 1 tensor(-0.4983) tensor(0.4992)\n",
      "176 0 tensor(-0.4983) tensor(0.4992)\n",
      "177 1 tensor(-0.4987) tensor(0.4997)\n",
      "178 1 tensor(-0.4970) tensor(0.4946)\n",
      "179 1 tensor(-0.4982) tensor(0.4996)\n",
      "180 1 tensor(-0.4997) tensor(0.4993)\n",
      "181 0 tensor(-0.4997) tensor(0.4993)\n",
      "182 1 tensor(-0.4996) tensor(0.4987)\n",
      "183 1 tensor(-0.4995) tensor(0.4977)\n",
      "184 1 tensor(-0.4988) tensor(0.4998)\n",
      "185 0 tensor(-0.4988) tensor(0.4998)\n",
      "186 1 tensor(-0.4994) tensor(0.4998)\n",
      "187 0 tensor(-0.4994) tensor(0.4998)\n",
      "188 0 tensor(-0.4994) tensor(0.4998)\n",
      "189 1 tensor(-0.4990) tensor(0.4954)\n",
      "190 0 tensor(-0.4990) tensor(0.4954)\n",
      "191 1 tensor(-0.4995) tensor(0.4994)\n",
      "192 0 tensor(-0.4995) tensor(0.4994)\n",
      "193 1 tensor(-0.4983) tensor(0.4999)\n",
      "194 1 tensor(-0.4995) tensor(0.4983)\n",
      "195 0 tensor(-0.4995) tensor(0.4983)\n",
      "196 0 tensor(-0.4995) tensor(0.4983)\n",
      "197 1 tensor(-0.4999) tensor(0.4953)\n",
      "198 1 tensor(-0.4985) tensor(0.4979)\n",
      "199 0 tensor(-0.4985) tensor(0.4979)\n",
      "200 1 tensor(-0.4982) tensor(0.4994)\n",
      "201 0 tensor(-0.4982) tensor(0.4994)\n",
      "202 0 tensor(-0.4982) tensor(0.4994)\n",
      "203 0 tensor(-0.4982) tensor(0.4994)\n",
      "204 0 tensor(-0.4982) tensor(0.4994)\n",
      "205 1 tensor(-0.4973) tensor(0.4997)\n",
      "206 1 tensor(-0.4994) tensor(0.4991)\n",
      "207 1 tensor(-0.4982) tensor(0.4986)\n",
      "208 0 tensor(-0.4982) tensor(0.4986)\n",
      "209 0 tensor(-0.4982) tensor(0.4986)\n",
      "210 1 tensor(-0.4998) tensor(0.4993)\n",
      "211 1 tensor(-0.4992) tensor(0.4999)\n",
      "212 0 tensor(-0.4992) tensor(0.4999)\n",
      "213 1 tensor(-0.4979) tensor(0.4947)\n",
      "214 1 tensor(-0.4997) tensor(0.4991)\n",
      "215 0 tensor(-0.4997) tensor(0.4991)\n",
      "216 0 tensor(-0.4997) tensor(0.4991)\n",
      "217 1 tensor(-0.4999) tensor(0.4992)\n",
      "218 0 tensor(-0.4999) tensor(0.4992)\n",
      "219 0 tensor(-0.4999) tensor(0.4992)\n",
      "220 0 tensor(-0.4999) tensor(0.4992)\n",
      "221 1 tensor(-0.4997) tensor(0.4996)\n",
      "222 0 tensor(-0.4997) tensor(0.4996)\n",
      "223 0 tensor(-0.4997) tensor(0.4997)\n",
      "224 1 tensor(-0.4983) tensor(0.4997)\n",
      "225 1 tensor(-0.4992) tensor(0.4980)\n",
      "226 0 tensor(-0.4992) tensor(0.4980)\n",
      "227 0 tensor(-0.4992) tensor(0.4980)\n",
      "228 0 tensor(-0.4992) tensor(0.4980)\n",
      "229 0 tensor(-0.4992) tensor(0.4980)\n",
      "230 1 tensor(-0.4991) tensor(0.4997)\n",
      "231 0 tensor(-0.4992) tensor(0.4997)\n",
      "232 0 tensor(-0.4992) tensor(0.4997)\n",
      "233 0 tensor(-0.4992) tensor(0.4997)\n",
      "234 0 tensor(-0.4991) tensor(0.4997)\n",
      "235 1 tensor(-0.4999) tensor(0.4998)\n",
      "236 0 tensor(-0.4999) tensor(0.4998)\n",
      "237 1 tensor(-0.4939) tensor(0.4968)\n",
      "238 0 tensor(-0.4939) tensor(0.4968)\n",
      "239 1 tensor(-0.4999) tensor(0.5000)\n",
      "240 0 tensor(-0.4999) tensor(0.5000)\n",
      "241 1 tensor(-0.4992) tensor(0.4987)\n",
      "242 1 tensor(-0.4964) tensor(0.4962)\n",
      "243 0 tensor(-0.4964) tensor(0.4962)\n",
      "244 1 tensor(-0.4996) tensor(0.4955)\n",
      "245 1 tensor(-0.4977) tensor(0.4960)\n",
      "246 0 tensor(-0.4977) tensor(0.4960)\n",
      "247 1 tensor(-0.4980) tensor(0.4979)\n",
      "248 0 tensor(-0.4980) tensor(0.4979)\n",
      "249 1 tensor(-0.4987) tensor(0.4988)\n",
      "250 0 tensor(-0.4987) tensor(0.4989)\n",
      "251 0 tensor(-0.4987) tensor(0.4988)\n",
      "252 0 tensor(-0.4987) tensor(0.4988)\n",
      "253 1 tensor(-0.4988) tensor(0.4997)\n",
      "254 1 tensor(-0.4984) tensor(0.4993)\n",
      "255 0 tensor(-0.4984) tensor(0.4993)\n",
      "256 1 tensor(-0.4991) tensor(0.4960)\n",
      "257 1 tensor(-0.4977) tensor(0.4970)\n",
      "258 0 tensor(-0.4977) tensor(0.4970)\n",
      "259 0 tensor(-0.4977) tensor(0.4970)\n",
      "260 0 tensor(-0.4977) tensor(0.4970)\n",
      "261 1 tensor(-0.4989) tensor(0.4922)\n",
      "262 0 tensor(-0.4989) tensor(0.4922)\n",
      "263 0 tensor(-0.4989) tensor(0.4922)\n",
      "264 0 tensor(-0.4989) tensor(0.4922)\n",
      "265 1 tensor(-0.4979) tensor(0.4997)\n",
      "266 1 tensor(-0.4990) tensor(0.4967)\n",
      "267 1 tensor(-0.4991) tensor(0.4997)\n",
      "268 1 tensor(-0.4963) tensor(0.4964)\n",
      "269 1 tensor(-0.4988) tensor(0.4959)\n",
      "270 0 tensor(-0.4988) tensor(0.4959)\n",
      "271 0 tensor(-0.4988) tensor(0.4959)\n",
      "272 0 tensor(-0.4988) tensor(0.4959)\n",
      "273 0 tensor(-0.4988) tensor(0.4959)\n",
      "274 0 tensor(-0.4988) tensor(0.4959)\n",
      "275 0 tensor(-0.4988) tensor(0.4959)\n",
      "276 1 tensor(-0.4987) tensor(0.4999)\n",
      "277 1 tensor(-0.4954) tensor(0.4991)\n",
      "278 0 tensor(-0.4954) tensor(0.4991)\n",
      "279 0 tensor(-0.4954) tensor(0.4991)\n",
      "280 1 tensor(-0.4991) tensor(0.4965)\n",
      "281 1 tensor(-0.4993) tensor(0.4964)\n",
      "282 1 tensor(-0.4971) tensor(0.4996)\n",
      "283 1 tensor(-0.4988) tensor(0.4989)\n",
      "284 0 tensor(-0.4988) tensor(0.4989)\n",
      "285 0 tensor(-0.4988) tensor(0.4989)\n",
      "286 1 tensor(-0.4984) tensor(0.4996)\n",
      "287 1 tensor(-0.4985) tensor(0.4970)\n",
      "288 1 tensor(-0.4955) tensor(0.4999)\n",
      "289 0 tensor(-0.4955) tensor(0.4999)\n",
      "290 1 tensor(-0.4985) tensor(0.4998)\n",
      "291 1 tensor(-0.4976) tensor(0.4996)\n",
      "292 1 tensor(-0.4994) tensor(0.4990)\n",
      "293 0 tensor(-0.4994) tensor(0.4990)\n",
      "294 1 tensor(-0.4989) tensor(0.4962)\n",
      "295 0 tensor(-0.4989) tensor(0.4962)\n",
      "296 0 tensor(-0.4989) tensor(0.4962)\n",
      "297 1 tensor(-0.4963) tensor(0.4991)\n",
      "298 0 tensor(-0.4963) tensor(0.4991)\n",
      "299 0 tensor(-0.4963) tensor(0.4991)\n",
      "300 0 tensor(-0.4963) tensor(0.4991)\n",
      "301 0 tensor(-0.4963) tensor(0.4991)\n",
      "302 1 tensor(-0.4981) tensor(0.4988)\n",
      "303 1 tensor(-0.4991) tensor(0.4994)\n",
      "304 1 tensor(-0.4992) tensor(0.4999)\n",
      "305 0 tensor(-0.4992) tensor(0.4999)\n",
      "306 0 tensor(-0.4992) tensor(0.4999)\n",
      "307 0 tensor(-0.4992) tensor(0.4999)\n",
      "308 0 tensor(-0.4992) tensor(0.4999)\n",
      "309 0 tensor(-0.4992) tensor(0.4999)\n",
      "310 1 tensor(-0.4967) tensor(0.4986)\n",
      "311 0 tensor(-0.4967) tensor(0.4986)\n",
      "312 0 tensor(-0.4966) tensor(0.4986)\n",
      "313 1 tensor(-0.4993) tensor(0.4995)\n",
      "314 1 tensor(-0.4998) tensor(0.4975)\n",
      "315 1 tensor(-0.4972) tensor(0.4999)\n",
      "316 0 tensor(-0.4972) tensor(0.4999)\n",
      "317 0 tensor(-0.4972) tensor(0.4999)\n",
      "318 0 tensor(-0.4972) tensor(0.4999)\n",
      "319 0 tensor(-0.4972) tensor(0.4999)\n",
      "320 0 tensor(-0.4972) tensor(0.4999)\n",
      "321 1 tensor(-0.4966) tensor(0.4994)\n",
      "322 0 tensor(-0.4966) tensor(0.4994)\n",
      "323 0 tensor(-0.4966) tensor(0.4994)\n",
      "324 0 tensor(-0.4966) tensor(0.4994)\n",
      "325 0 tensor(-0.4966) tensor(0.4994)\n",
      "326 0 tensor(-0.4966) tensor(0.4994)\n",
      "327 0 tensor(-0.4966) tensor(0.4994)\n",
      "328 1 tensor(-0.4970) tensor(0.4992)\n",
      "329 1 tensor(-0.4975) tensor(0.4962)\n",
      "330 0 tensor(-0.4975) tensor(0.4962)\n",
      "331 1 tensor(-0.4990) tensor(0.4999)\n",
      "332 0 tensor(-0.4990) tensor(0.4999)\n",
      "333 0 tensor(-0.4990) tensor(0.4999)\n",
      "334 1 tensor(-0.4989) tensor(0.4991)\n",
      "335 1 tensor(-0.4998) tensor(0.4991)\n",
      "336 0 tensor(-0.4998) tensor(0.4991)\n",
      "337 0 tensor(-0.4998) tensor(0.4991)\n",
      "338 1 tensor(-0.4992) tensor(0.4984)\n",
      "339 1 tensor(-0.4995) tensor(0.4995)\n",
      "340 0 tensor(-0.4995) tensor(0.4995)\n",
      "341 1 tensor(-0.4993) tensor(0.4983)\n",
      "342 1 tensor(-0.4993) tensor(0.4981)\n",
      "343 1 tensor(-0.5000) tensor(0.4988)\n",
      "344 1 tensor(-0.4986) tensor(0.4997)\n",
      "345 0 tensor(-0.4986) tensor(0.4997)\n",
      "346 0 tensor(-0.4986) tensor(0.4997)\n",
      "347 0 tensor(-0.4986) tensor(0.4997)\n",
      "348 0 tensor(-0.4986) tensor(0.4997)\n",
      "349 0 tensor(-0.4986) tensor(0.4997)\n",
      "350 0 tensor(-0.4986) tensor(0.4996)\n",
      "351 1 tensor(-0.4969) tensor(0.4999)\n",
      "352 1 tensor(-0.4997) tensor(0.4998)\n",
      "353 0 tensor(-0.4997) tensor(0.4998)\n",
      "354 1 tensor(-0.4999) tensor(0.4995)\n",
      "355 1 tensor(-0.4970) tensor(0.4976)\n",
      "356 0 tensor(-0.4970) tensor(0.4976)\n",
      "357 1 tensor(-0.4990) tensor(0.4985)\n",
      "358 0 tensor(-0.4990) tensor(0.4985)\n",
      "359 0 tensor(-0.4990) tensor(0.4985)\n",
      "360 1 tensor(-0.4992) tensor(0.4995)\n",
      "361 1 tensor(-0.4998) tensor(0.4966)\n",
      "362 1 tensor(-0.4994) tensor(0.4984)\n",
      "363 1 tensor(-0.4993) tensor(0.4986)\n",
      "364 0 tensor(-0.4993) tensor(0.4986)\n",
      "365 0 tensor(-0.4993) tensor(0.4986)\n",
      "366 1 tensor(-0.4999) tensor(0.4996)\n",
      "367 1 tensor(-0.4990) tensor(0.4987)\n",
      "368 1 tensor(-0.4991) tensor(0.4997)\n",
      "369 0 tensor(-0.4991) tensor(0.4997)\n",
      "370 0 tensor(-0.4991) tensor(0.4997)\n",
      "371 1 tensor(-0.5000) tensor(0.5000)\n",
      "372 1 tensor(-0.4998) tensor(0.4946)\n",
      "373 1 tensor(-0.4995) tensor(0.4958)\n",
      "374 1 tensor(-0.4988) tensor(0.4988)\n",
      "375 1 tensor(-0.4991) tensor(0.5000)\n",
      "376 1 tensor(-0.4974) tensor(0.4997)\n",
      "377 1 tensor(-0.4946) tensor(0.4969)\n",
      "378 0 tensor(-0.4946) tensor(0.4969)\n",
      "379 0 tensor(-0.4946) tensor(0.4969)\n",
      "380 0 tensor(-0.4946) tensor(0.4969)\n",
      "381 1 tensor(-0.4993) tensor(0.4998)\n",
      "382 1 tensor(-0.4997) tensor(0.5000)\n",
      "383 0 tensor(-0.4997) tensor(0.5000)\n",
      "384 1 tensor(-0.4979) tensor(0.4999)\n",
      "385 1 tensor(-0.4940) tensor(0.4983)\n",
      "386 1 tensor(-0.4977) tensor(0.4995)\n",
      "387 0 tensor(-0.4977) tensor(0.4995)\n",
      "388 1 tensor(-0.5000) tensor(0.4966)\n",
      "389 1 tensor(-0.4978) tensor(0.4995)\n",
      "390 1 tensor(-0.4944) tensor(0.4986)\n",
      "391 0 tensor(-0.4944) tensor(0.4986)\n",
      "392 1 tensor(-0.4998) tensor(0.4975)\n",
      "393 1 tensor(-0.4985) tensor(0.4966)\n",
      "394 1 tensor(-0.4988) tensor(0.4978)\n",
      "395 0 tensor(-0.4988) tensor(0.4978)\n",
      "396 0 tensor(-0.4988) tensor(0.4978)\n",
      "397 0 tensor(-0.4988) tensor(0.4978)\n",
      "398 0 tensor(-0.4988) tensor(0.4978)\n",
      "399 1 tensor(-0.4976) tensor(0.4992)\n",
      "400 0 tensor(-0.4976) tensor(0.4992)\n",
      "401 0 tensor(-0.4976) tensor(0.4991)\n",
      "402 0 tensor(-0.4976) tensor(0.4991)\n",
      "403 1 tensor(-0.4990) tensor(0.4964)\n",
      "404 0 tensor(-0.4990) tensor(0.4964)\n",
      "405 1 tensor(-0.4980) tensor(0.4977)\n",
      "406 1 tensor(-0.4991) tensor(0.4988)\n",
      "407 1 tensor(-0.4960) tensor(0.4999)\n",
      "408 1 tensor(-0.4998) tensor(0.4990)\n",
      "409 0 tensor(-0.4998) tensor(0.4990)\n",
      "410 1 tensor(-0.4950) tensor(0.4976)\n",
      "411 0 tensor(-0.4950) tensor(0.4976)\n",
      "412 1 tensor(-0.4989) tensor(0.4988)\n",
      "413 0 tensor(-0.4988) tensor(0.4988)\n",
      "414 1 tensor(-0.4953) tensor(0.4996)\n",
      "415 1 tensor(-0.4995) tensor(0.4985)\n",
      "416 0 tensor(-0.4995) tensor(0.4985)\n",
      "417 1 tensor(-0.4995) tensor(0.4989)\n",
      "418 0 tensor(-0.4995) tensor(0.4989)\n",
      "419 1 tensor(-0.4976) tensor(0.4984)\n",
      "420 0 tensor(-0.4976) tensor(0.4984)\n",
      "421 0 tensor(-0.4976) tensor(0.4984)\n",
      "422 0 tensor(-0.4976) tensor(0.4984)\n",
      "423 1 tensor(-0.4974) tensor(0.4969)\n",
      "424 1 tensor(-0.5000) tensor(0.4999)\n",
      "425 1 tensor(-0.4987) tensor(0.4997)\n",
      "426 0 tensor(-0.4987) tensor(0.4997)\n",
      "427 0 tensor(-0.4987) tensor(0.4997)\n",
      "428 0 tensor(-0.4987) tensor(0.4997)\n",
      "429 1 tensor(-0.4976) tensor(0.4977)\n",
      "430 1 tensor(-0.4981) tensor(0.4979)\n",
      "431 0 tensor(-0.4981) tensor(0.4980)\n",
      "432 0 tensor(-0.4981) tensor(0.4980)\n",
      "433 1 tensor(-0.4957) tensor(0.4975)\n",
      "434 1 tensor(-0.4995) tensor(0.4988)\n",
      "435 0 tensor(-0.4995) tensor(0.4988)\n",
      "436 0 tensor(-0.4995) tensor(0.4988)\n",
      "437 1 tensor(-0.4984) tensor(0.4990)\n",
      "438 1 tensor(-0.4978) tensor(0.4978)\n",
      "439 0 tensor(-0.4978) tensor(0.4978)\n",
      "440 0 tensor(-0.4978) tensor(0.4978)\n",
      "441 0 tensor(-0.4978) tensor(0.4978)\n",
      "442 0 tensor(-0.4978) tensor(0.4978)\n",
      "443 0 tensor(-0.4978) tensor(0.4978)\n",
      "444 0 tensor(-0.4978) tensor(0.4978)\n",
      "445 0 tensor(-0.4978) tensor(0.4978)\n",
      "446 1 tensor(-0.4998) tensor(0.4991)\n",
      "447 0 tensor(-0.4997) tensor(0.4991)\n",
      "448 1 tensor(-0.4999) tensor(0.4971)\n",
      "449 1 tensor(-0.4972) tensor(0.4998)\n",
      "450 0 tensor(-0.4972) tensor(0.4998)\n",
      "451 1 tensor(-0.4982) tensor(0.4953)\n",
      "452 0 tensor(-0.4982) tensor(0.4953)\n",
      "453 1 tensor(-0.4973) tensor(0.4982)\n",
      "454 1 tensor(-0.4977) tensor(0.4998)\n",
      "455 1 tensor(-0.4950) tensor(0.4976)\n",
      "456 1 tensor(-0.4990) tensor(0.4999)\n",
      "457 0 tensor(-0.4990) tensor(0.4999)\n",
      "458 0 tensor(-0.4990) tensor(0.4999)\n",
      "459 0 tensor(-0.4990) tensor(0.4999)\n",
      "460 0 tensor(-0.4990) tensor(0.4999)\n",
      "461 0 tensor(-0.4990) tensor(0.4999)\n",
      "462 1 tensor(-0.4979) tensor(0.4984)\n",
      "463 1 tensor(-0.4995) tensor(0.4953)\n",
      "464 0 tensor(-0.4995) tensor(0.4953)\n",
      "465 1 tensor(-0.4999) tensor(0.4988)\n",
      "466 1 tensor(-0.4994) tensor(0.4999)\n",
      "467 1 tensor(-0.4987) tensor(0.4980)\n",
      "468 1 tensor(-0.4991) tensor(0.4985)\n",
      "469 0 tensor(-0.4991) tensor(0.4985)\n",
      "470 0 tensor(-0.4991) tensor(0.4985)\n",
      "471 0 tensor(-0.4991) tensor(0.4985)\n",
      "472 1 tensor(-0.4952) tensor(0.5000)\n",
      "473 1 tensor(-0.4999) tensor(0.4975)\n",
      "474 0 tensor(-0.4999) tensor(0.4975)\n",
      "475 1 tensor(-0.4982) tensor(0.4988)\n",
      "476 0 tensor(-0.4982) tensor(0.4988)\n",
      "477 0 tensor(-0.4982) tensor(0.4988)\n",
      "478 0 tensor(-0.4982) tensor(0.4988)\n",
      "479 0 tensor(-0.4982) tensor(0.4988)\n",
      "480 0 tensor(-0.4982) tensor(0.4988)\n",
      "481 1 tensor(-0.4993) tensor(0.4991)\n",
      "482 1 tensor(-0.4970) tensor(0.4984)\n",
      "483 1 tensor(-0.4988) tensor(0.4993)\n",
      "484 1 tensor(-0.4992) tensor(0.4964)\n",
      "485 1 tensor(-0.4999) tensor(0.4976)\n",
      "486 1 tensor(-0.4970) tensor(0.4971)\n",
      "487 0 tensor(-0.4970) tensor(0.4971)\n",
      "488 0 tensor(-0.4970) tensor(0.4971)\n",
      "489 0 tensor(-0.4970) tensor(0.4971)\n",
      "490 0 tensor(-0.4970) tensor(0.4971)\n",
      "491 1 tensor(-0.4980) tensor(0.4989)\n",
      "492 1 tensor(-0.4998) tensor(0.4946)\n",
      "493 1 tensor(-0.4990) tensor(0.4998)\n",
      "494 0 tensor(-0.4990) tensor(0.4998)\n",
      "495 0 tensor(-0.4990) tensor(0.4998)\n",
      "496 0 tensor(-0.4990) tensor(0.4998)\n",
      "497 0 tensor(-0.4990) tensor(0.4998)\n",
      "498 0 tensor(-0.4990) tensor(0.4998)\n",
      "499 0 tensor(-0.4990) tensor(0.4998)\n",
      "500 0 tensor(-0.4990) tensor(0.4998)\n",
      "501 0 tensor(-0.4990) tensor(0.4998)\n",
      "502 0 tensor(-0.4990) tensor(0.4998)\n",
      "503 1 tensor(-0.4998) tensor(0.4981)\n",
      "504 1 tensor(-0.4983) tensor(0.4981)\n",
      "505 1 tensor(-0.4970) tensor(0.4983)\n",
      "506 0 tensor(-0.4970) tensor(0.4983)\n",
      "507 0 tensor(-0.4970) tensor(0.4983)\n",
      "508 0 tensor(-0.4970) tensor(0.4983)\n",
      "509 1 tensor(-0.4993) tensor(0.4995)\n",
      "510 0 tensor(-0.4993) tensor(0.4995)\n",
      "511 1 tensor(-0.4996) tensor(0.4991)\n"
     ]
    }
   ],
   "source": [
    "# for debug\n",
    "accfft_cpy = accfft.clone()\n",
    "alpha = ctsum[1:]\n",
    "for i in range(n):\n",
    "    ai = alpha[i]\n",
    "    accfft += alphapoly[ai] * rgswmult(accfft, brk[i])\n",
    "    accfft_cpy += alphapoly[ai] * (accfft_cpy * sk[i])\n",
    "\n",
    "    print(i, int(sk[i]), torch.min(decrypt_from_fft(accfft - accfft_cpy, skNfft)/Q), torch.max(decrypt_from_fft(accfft - accfft_cpy, skNfft)/Q))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha = ctsum[1:]\n",
    "for i in range(n):\n",
    "    ai = alpha[i]\n",
    "    accfft += alphapoly[ai] * rgswmult(accfft, brk[i])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "alpha = ctsum[1:]\n",
    "for i in range(n):\n",
    "    ai = alpha[i]\n",
    "    accfft += alphapoly[ai] * (accfft * sk[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = negacyclic_ifft(accfft, N, Q)\n",
    "accLWE = extract(acc)\n",
    "accLWE[0] += (Q >> 3)\n",
    "accLWE &= Q - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND output: 1\n",
      "encrypted result: 1\n"
     ]
    }
   ],
   "source": [
    "# Calculate NAND output\n",
    "print(f\"NAND output: {int(not (m0 and m1))}\")\n",
    "print(f\"encrypted result: {decryptLWE(accLWE, skN, Q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we want from blind rotation\n",
    "sa = torch.dot(ctsum[1:], sk) & (q-1)\n",
    "\n",
    "xsa = torch.zeros([N])\n",
    "if sa < N:\n",
    "    xsa[sa] = 1\n",
    "else:\n",
    "    xsa[sa - N] = -1\n",
    "\n",
    "accfft *= negacyclic_fft(xsa, N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = negacyclic_ifft(accfft, N, Q)\n",
    "accLWE = extract(acc)\n",
    "accLWE[0] += (Q >> 3)\n",
    "accLWE &= Q - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND output: 1\n",
      "encrypted result: 2\n"
     ]
    }
   ],
   "source": [
    "# Calculate NAND output\n",
    "print(f\"NAND output: {int(not (m0 and m1))}\")\n",
    "print(f\"encrypted result: {decryptLWE(accLWE, skN, Q)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following codes are what we actually do in blind rotation (explained in unencrypted way)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "testacc = torch.arange(N)\n",
    "testaccfft = negacyclic_fft(testacc, N, Q)\n",
    "\n",
    "alpha = ctsum[1:]\n",
    "for i in range(n):\n",
    "    ai = alpha[i]\n",
    "    testaccfft += alphapoly[ai] * (testaccfft * sk[i])\n",
    "\n",
    "testaccres = negacyclic_ifft(testaccfft, N, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 865,  866,  867,  ..., 1186, 1185, 1184], dtype=torch.int32)"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testaccres &= (2*N-1)\n",
    "testaccres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1182, dtype=torch.int32)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "beta = ctsum[0]\n",
    "alpha = ctsum[1:]\n",
    "sa = torch.dot(alpha, sk)\n",
    "\n",
    "(sa) & (q-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sa) & (q-1) < N:\n",
    "    print(testaccres[(sa) & (q-1):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
