{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LWE and RLWE encryption\n",
    "## LWE encryption\n",
    "\n",
    "In FHEW-like HE, we use both LWE and RLWE encryption.\n",
    "LWE is for a ciphertext and RLWE is for the core part of bootstrapping so called *blind rotation*.\n",
    "\n",
    "We define LWE encryption as follows\n",
    "$$\\texttt{LWE}_{\\vec{s}, n, q}(m) = (\\beta, \\vec{\\alpha}) \\in \\mathbb{Z}_q^{n+1},$$\n",
    "where $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right> \\in \\mathbb{Z}_q$, and $\\vec{s} \\leftarrow \\chi_{key}$ is a secret key and $\\vec{e} \\leftarrow \\chi_{err}$ is a added noise for security.\n",
    "$\\vec{\\alpha}$ is unifromly sampled in $\\mathbb{Z}_q^n$.\n",
    "\n",
    "Here, we choose $\\chi_{key}$ as binary distribution and $\\chi_{err}$ as a Gaussian distribution with standard deviation $3.2$.\n",
    "\n",
    "Let's make the encryption method. We use pytorch for easy and fast implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use parameter sets $(n, q, \\sigma) = (512, 2048, 3.2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = 3.2\n",
    "n = 512\n",
    "q = 2048"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are generator of key and error.\n",
    "\n",
    "NOTE: Those generators are not secure. You should **NOT** use them in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keygen(dim):\n",
    "    return torch.randint(2, size = (dim,))\n",
    "def errgen(stddev):\n",
    "    e = torch.round(stddev*torch.randn(1))\n",
    "    e = e.squeeze()\n",
    "    return e.to(torch.int)\n",
    "def uniform(dim, modulus):\n",
    "    return torch.randint(modulus, size = (dim,))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first generate the secret key $\\vec{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = keygen(n)\n",
    "s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encrypt, we need random part $\\vec{\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 213, 1767, 1877,  371,  535, 1617,  481, 1841, 1424, 1921, 1704,  800,\n",
       "        1873,   35,  217,  687,  872,  755, 1019, 1272,  444,  775,   80, 1844,\n",
       "        1597,  652,  728, 1909, 1831,  798,  636,  806, 1776,  594, 1292, 1783,\n",
       "        1084,  410, 1416, 2038,  235, 1397, 1629,  873, 1923, 1968,  726,  353,\n",
       "         738,  520, 1769,   33,  292, 1734, 1438, 1150, 1908, 1113,  559, 1860,\n",
       "         557, 1625, 1118, 1276, 1112, 1366,  547, 1497,  326, 1450, 1693, 1830,\n",
       "        1721, 2012,  856, 1210,  306,  802,  160,  741, 1966, 1618, 1360,   75,\n",
       "        1708, 1360, 1561,  900,  693, 1770,   49,  920,  631, 1865,  273, 1230,\n",
       "        1111, 1516,  857, 1562, 1663,  182, 1356,   47,   49, 1253, 1196, 1963,\n",
       "         513,   51, 1298,  415,  104, 1967,   75,  920, 1841,  159,  950,   79,\n",
       "        1579, 1875, 1992,  470, 1858, 1880, 1223,   23, 1844,  619,   45, 1749,\n",
       "         100,  401,  629, 2036,  674,  437, 1726, 2004,  217,  254, 1491, 1836,\n",
       "         757,  158,  602,  621,  267,  673, 1292,  172,   96,  996, 1162, 1074,\n",
       "        1480, 1176,  690,  243,  274,  894, 1353, 1220, 1935,  879, 1027,  370,\n",
       "        1491, 1109, 1364,  482,  448,  462,  390, 1703, 1726,  242, 1419,  569,\n",
       "        1062,  563,  160, 1115,  664,  283,   55,  918,   91,  992, 1708, 1478,\n",
       "         305, 1774,  847, 1241,  522,  577, 1062,  919, 1597, 1517, 1482,  396,\n",
       "        1790,  277, 1420,  110, 1035, 1842,  712,  858,  483,  911,  793, 1691,\n",
       "        1792, 1736, 1259, 1604, 1644,  439,  144,  169,  644,  528, 1863, 1538,\n",
       "        1730,  304, 1701, 1453, 1896, 1697, 1776,  215,   14, 1329, 1076,  465,\n",
       "         546, 1057, 1214, 2013,  458,  287,   97,  905,  429, 1592, 1342,  847,\n",
       "        1364, 1176, 1930,  223, 1155,  539,  261,  428,  830,  630,  975,   56,\n",
       "         668, 1755,  581, 1578, 1216,  391, 1504, 1347, 1148,  576, 1878,  278,\n",
       "        1393, 1228,  774,  921, 1162,  613,  550,  284,  739,  731, 1004, 1642,\n",
       "        1072,  951, 1481,  516,  411,  838,   84,  271,  979,  227,  113, 1572,\n",
       "        1308,  958, 1383,  974, 1556,    8,  311, 1579, 1267,  136,  332, 1485,\n",
       "          56, 1077, 1695,  162,  668, 1878, 1373,  495,  725,  261,  129,  233,\n",
       "         760, 1063, 1455,  349, 2022,  992,  651, 1621,   52,  454,  876,  314,\n",
       "        1352, 1098, 1542,   65, 1754,  161,  446, 1582,  779, 1143, 1275, 1410,\n",
       "        1203,  960,  435,  840, 1296,  977, 1409,  355, 1610,  293,  622,  555,\n",
       "         615,   18, 1139, 1143, 1108, 1363, 1123,  443, 1209,  965, 1434,  431,\n",
       "         808, 1975,  596,  685, 1561, 1150,  298, 1928, 1502,  730,  493, 1179,\n",
       "         706,  545,  265, 1639,   20, 1863,  858,   99, 1010,    7,  379, 1280,\n",
       "         871,  978,  172,  191, 1839, 1766, 1920, 1704,  726, 1719,  974,  164,\n",
       "         328, 1448, 1693,  202,  285,  669, 1146,  946,  214,  457,  683,  331,\n",
       "         217, 1849, 1305,   59, 1639,  159,   20, 1434,   35,  314, 1229,  710,\n",
       "        1118,   75,  877,  252, 1966,  266,  592, 1562,  552,  677,  289, 1082,\n",
       "        1004,  744, 1018, 1085,  874, 1801, 1003,  102, 1837, 1900, 2044,  224,\n",
       "        1489, 1175, 1610,  737, 1002,  202, 1510,  428,  536,  386, 1431, 1415,\n",
       "        1468,  508, 1987, 1513,  391, 1706,  336, 1987,  125,  267, 1491,  781,\n",
       "        1472,  584, 1049,  664,  303, 1181, 1992,  819, 1527,  628,  348,  617,\n",
       "        1335, 1070,   85,  325,  116,  247, 1524,  600,  451,  909,  428, 1248,\n",
       "        1596,  431, 1491, 1588, 1704,  868, 1697,  644])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = uniform(n, q)\n",
    "alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calaulate $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$ for encryption.\n",
    "\n",
    "Let the message we are encrypting is a binary value e,g, $m = 1$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1808)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "\n",
    "beta %= q\n",
    "\n",
    "beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decrypt the ciphertext above.\n",
    "\n",
    "As $\\beta = m + e - \\left< \\vec{\\alpha}, \\vec{s} \\right>$, we can find $m + e = \\beta + \\left< \\vec{\\alpha}, \\vec{s} \\right>$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are very lucky, you might get the decrypted value.\n",
    "```\n",
    ">>> m_decrypted\n",
    "tensor(1)\n",
    "```\n",
    "But, if you run the code once, you will get other value.\n",
    "Note here that we get $m+e$ by decryption not exactly the value $m$.\n",
    "\n",
    "To make our message safe from the error, we can multiply certain *scaling factor* to our message.\n",
    "\n",
    "Here, let's multiply $q/4$, and encrypt/decrypt again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(511)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1\n",
    "# multiply scaling factor q/4 \n",
    "m *= q//4\n",
    "\n",
    "beta = m - torch.dot(alpha, s)\n",
    "e = errgen(stddev)\n",
    "beta += e\n",
    "beta %= q\n",
    "\n",
    "m_decrypted = beta + torch.dot(alpha, s)\n",
    "m_decrypted %= q\n",
    "\n",
    "m_decrypted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a value near $m \\cdot q/4 = 512$.\n",
    "Division by $q/4$ and rounding will give us original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescale the message\n",
    "m_decrypted = m_decrypted.to(torch.float)\n",
    "m_decrypted /= q/4.\n",
    "m_decrypted = torch.round(m_decrypted)\n",
    "m_decrypted.to(torch.int)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decryption is successful!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LWE encryption function\n",
    "\n",
    "By *LWE assumption* $\\beta$ should look like a random value.\n",
    "\n",
    "The LWE ciphertext is a pair $(\\beta, \\vec{\\alpha})$.\n",
    "\n",
    "We define the encryptor as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(478),\n",
       " tensor([ 144, 1380,  710,  783,  194,  497,  997,  215,   13, 1299,  764, 1392,\n",
       "         1718, 1799,  502, 1746, 1747,  134,  751,  752,  634, 1738, 1121,  680,\n",
       "          887, 1544,   85,  224,  224,  694, 1211, 1571, 1703, 1191,  835, 1394,\n",
       "         1447,  909, 2027, 1049, 1164,  276,  700, 1668, 1330, 1691,  968, 1359,\n",
       "         1805,   79, 1940,  444,  611,   90,  344, 1451, 1279,  799, 1161, 1942,\n",
       "           98, 1660,  620, 2037, 1843,  563,  821, 1139, 1439, 1978, 1723,  157,\n",
       "         1773, 1718,  747,  716,  319, 1616, 1321,  864, 1705, 1670,  896, 1423,\n",
       "          663,  933, 1399, 1944,  328, 1512,  870,  129,  640,  172,  475,  177,\n",
       "         1600, 1623, 1887, 1785,  825, 1975, 1431,  813, 1276,  394, 1794,  635,\n",
       "          884,  415, 1684, 1709, 1042, 1549,  254,  238,  939,   20,  684,  273,\n",
       "          114, 1282,  851,    7, 1934, 1014,  213,  996, 1263, 1748, 1232, 1872,\n",
       "          935,  234,  398, 1427,  276,   31,  188, 2040,  790, 1528,  305,  510,\n",
       "         1815, 1194, 1159, 1228,  667, 1853, 1153, 1141,  466, 1052, 1041,  350,\n",
       "         1555,  982, 1130,  751, 1190, 1098,  239,  223, 1885, 1382, 1766,  615,\n",
       "          976, 1055,  741,  408,  659, 1325,  685, 1742, 1797,  514,  132, 1006,\n",
       "          889,  894,  321,  755,  268, 1272, 2027,  976,   13,  514, 1299,  160,\n",
       "         1668, 1988, 1682,  155,   79, 1617,   60,   80,  597,  713, 1427,  300,\n",
       "         1229, 1063, 1245,   56,  684,  207,  470,  210, 1770, 1942, 1890,   78,\n",
       "         1717, 1952, 1555, 1223, 2015, 1526,  826,  153,  412, 1032, 2019, 2031,\n",
       "         1746, 1209, 1013, 1157, 1874,  212,  422,  287,  613,  377, 1237,  342,\n",
       "         1306,  915,  186, 1611, 1877,  808,  992,  793,  339, 1517, 1815,  817,\n",
       "          678, 1537, 2036,  576, 1373, 1577,  254, 1253,  139,  595,  777,   72,\n",
       "          810, 1455, 1236, 1831, 1573, 1446,  577, 1528, 1165, 1839, 1033,  109,\n",
       "         1583, 1676, 1040,  916, 1850,   72, 1233,  270, 1630,  351, 1910, 1628,\n",
       "         1698, 1403, 1036,  970, 1196,  954, 1407, 1649,  928, 1639,   90,  773,\n",
       "          794, 1014, 1369, 1512,    3,  988,  524,  421,  747, 1129, 1586, 1177,\n",
       "         2028, 1831, 1303, 1545,   60, 1641,   54, 1042, 1952,  307, 1521, 1242,\n",
       "         1987,  604, 1129, 1967, 1904, 1403,  875, 1147,  326, 1403,  525,  113,\n",
       "         1376, 1294, 1240,  419, 1888,   54,  675,  962,  463,  765, 1798, 1950,\n",
       "         1399, 1339, 1329,  817, 1123,  493, 1053,  375, 1284,  528, 1175,  129,\n",
       "          233, 1350,  188,  465, 2031, 1670,  537,  226,  928,  385, 1858,  584,\n",
       "          265,  603,  712, 1174,  229,  508,  837,  251,  569, 1207,  274, 1043,\n",
       "         1963, 1018,  952,  478, 1099,  190, 1234, 1666, 1330, 1629,  232, 1005,\n",
       "         1621, 1677,  771, 1928,  889, 1970,  721,  409,  145,  514, 1189,  838,\n",
       "         1746, 1570,   83, 1670,  174,  557,  989,   61, 2009, 1305, 1396, 1411,\n",
       "          121, 1205, 1187,  589,  552,  528, 1482, 1506,  907, 1903, 1939,  384,\n",
       "          498, 1284,  164, 1552, 1958,   39,  894, 1927, 1506, 1138,  828,  274,\n",
       "         1757,   27,  868,  657, 2027, 1729,  167,  464, 1305, 1390,  755,  430,\n",
       "         1053,  670, 1901,  522,  922, 1641,   65, 1903,  677,  127,  884,   51,\n",
       "         1446, 1776, 1007,   43,  585,  778, 1715,  511,  902,  541, 1994, 1942,\n",
       "         1799, 1145, 1411,  497,    4, 1474,  290, 1476,   48,  913,  987,  331,\n",
       "          433,  933, 1218, 1056,  197, 1073,  515,  102, 1956,  722, 1870, 1694,\n",
       "          349,  213,  514,  992,   98, 1228, 1036, 1044]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encryptLWE(message, dim, modulus, key):\n",
    "    alpha = uniform(n, modulus)\n",
    "\n",
    "    beta = message - torch.dot(alpha, s)\n",
    "    e = errgen(stddev)\n",
    "    beta += e\n",
    "    beta %= modulus\n",
    "\n",
    "    return (beta, alpha)\n",
    "\n",
    "\n",
    "ct = encryptLWE(1, n, q, s)\n",
    "ct    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
